{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This page provides documentation for analysis tools developed by the Connectivity Map Project at the Broad Institute . The focus of the CMap project is to create and analyze large perturbational datasets to aid our understanding of human disease and to accelerate the discovery of novel therapeutics. To learn more about CMap visit clue.io","title":"Home"},{"location":"docker_demo/","text":"This tutorial demonstrates analyzing custom datasets using dockerized sig-tools. Pre-requisites \u00b6 Install Docker for your platform from docker.com Pull the CMap SigTool Runtime Docker image (optional) docker pull cmap/sigtool-runtime Clone the cmap-sig-tools repo git clone https://github.com/cmap/cmap-sig-tools Download and extract the tools and test data cd cmap-sig-tools/demo sh ./get-demo.sh Demos \u00b6 1. Run a Cmap Query against an L1000 dataset using the QueryL1k tool \u00b6 datasets/examples/01_run_query.sh This script computes a set-based enrichment similarity between input genesets (aka queries) and a small subset of L1000 perturbational gene-expression signatures. (Note that while the tool is optimized for datasets generated by the L1000 platform, any perturbational dataset can be used). The algorithm operates as follows. First raw similarity (connectivity) scores between a query and CMap signatures are computed. While query methodology is agnostic to the specific similarity metric, the default choice is a non-parametric, two-tailed weighted gene-set enrichment score (Subramanian, A. et al. Cell 2017). The raw scores are then scaled (normalized) by the signed-means to allow for comparisons across different queries. Finally the statistical significance of the connections adjusted for multiple hypotheses is estimated. FDR q-values are estimated by comparing the distributions of treatments to null signatures in the dataset. Outputs: the tool produces the following output (in the results folder) arfs/ : Per-query analysis report files (ARFs) <QUERY_NAME>/query_result.gct : a GCT format text file listing the annotations, connectivity scores and q-values for each signature in the dataset. The following fields are computed by the query tool: raw_cs : Raw connectivity scores norm_cs : Normalized connectivity score computed by dividing the raw connectivity scores by the signed-mean scores of signatures (specified by the is_ncs_sig field in the signature metadata file) If the ncs_group field is not empty the scores are normalized within each group, otherwise the scores are normalized using the global means across all signatures. fdr_q_nlog10 : Negative log10 transformed FDR q-values estimated relative to the null signatures (specified by the is_null_sig field in the signature annotation file). matrices/query : Query parameters and result matrices in GCTx format for all queries: up.gmt , dn.gmt : query genesets in GMT format cs.gctx : Raw connectivity scores matrix [signatures x queries] ncs.gctx : Normalized connectivity score matrix [signatures x queries] fdr_qvalue.gctx : Estimated false discovery rate q-values [signatures x queries] 2. Test enrichment of user-defined sets using the GSEA Preranked tool \u00b6 datasets/examples/03_run_gseapreranked.sh The GSEA Preranked tool computes set-based enrichment analysis against a user-defined rank-ordered dataset. It determines whether a priori defined sets show statistically significant enrichment at either end of the ranking. The demo script to test for enrichment of pre-defined groupings of signatures 3. Use the Curie tool to query viability data using cell-sets \u00b6 datasets/examples/03_run_curie.sh The Curie tool computes a set-based enrichment similarity between input cell-line sets (aka queries) and a perturbational cell-fitness signature dataset. Note that while the tool is optimized for datasets generated by the PRISM platform, any high-dimensional cell-fitness dataset can be used.","title":"Running Dockerized SigTools"},{"location":"docker_demo/#pre-requisites","text":"Install Docker for your platform from docker.com Pull the CMap SigTool Runtime Docker image (optional) docker pull cmap/sigtool-runtime Clone the cmap-sig-tools repo git clone https://github.com/cmap/cmap-sig-tools Download and extract the tools and test data cd cmap-sig-tools/demo sh ./get-demo.sh","title":"Pre-requisites"},{"location":"docker_demo/#demos","text":"","title":"Demos"},{"location":"docker_demo/#1-run-a-cmap-query-against-an-l1000-dataset-using-the-queryl1k-tool","text":"datasets/examples/01_run_query.sh This script computes a set-based enrichment similarity between input genesets (aka queries) and a small subset of L1000 perturbational gene-expression signatures. (Note that while the tool is optimized for datasets generated by the L1000 platform, any perturbational dataset can be used). The algorithm operates as follows. First raw similarity (connectivity) scores between a query and CMap signatures are computed. While query methodology is agnostic to the specific similarity metric, the default choice is a non-parametric, two-tailed weighted gene-set enrichment score (Subramanian, A. et al. Cell 2017). The raw scores are then scaled (normalized) by the signed-means to allow for comparisons across different queries. Finally the statistical significance of the connections adjusted for multiple hypotheses is estimated. FDR q-values are estimated by comparing the distributions of treatments to null signatures in the dataset. Outputs: the tool produces the following output (in the results folder) arfs/ : Per-query analysis report files (ARFs) <QUERY_NAME>/query_result.gct : a GCT format text file listing the annotations, connectivity scores and q-values for each signature in the dataset. The following fields are computed by the query tool: raw_cs : Raw connectivity scores norm_cs : Normalized connectivity score computed by dividing the raw connectivity scores by the signed-mean scores of signatures (specified by the is_ncs_sig field in the signature metadata file) If the ncs_group field is not empty the scores are normalized within each group, otherwise the scores are normalized using the global means across all signatures. fdr_q_nlog10 : Negative log10 transformed FDR q-values estimated relative to the null signatures (specified by the is_null_sig field in the signature annotation file). matrices/query : Query parameters and result matrices in GCTx format for all queries: up.gmt , dn.gmt : query genesets in GMT format cs.gctx : Raw connectivity scores matrix [signatures x queries] ncs.gctx : Normalized connectivity score matrix [signatures x queries] fdr_qvalue.gctx : Estimated false discovery rate q-values [signatures x queries]","title":"1. Run a Cmap Query against an L1000 dataset using the QueryL1k tool"},{"location":"docker_demo/#2-test-enrichment-of-user-defined-sets-using-the-gsea-preranked-tool","text":"datasets/examples/03_run_gseapreranked.sh The GSEA Preranked tool computes set-based enrichment analysis against a user-defined rank-ordered dataset. It determines whether a priori defined sets show statistically significant enrichment at either end of the ranking. The demo script to test for enrichment of pre-defined groupings of signatures","title":"2. Test enrichment of user-defined sets using the GSEA Preranked tool"},{"location":"docker_demo/#3-use-the-curie-tool-to-query-viability-data-using-cell-sets","text":"datasets/examples/03_run_curie.sh The Curie tool computes a set-based enrichment similarity between input cell-line sets (aka queries) and a perturbational cell-fitness signature dataset. Note that while the tool is optimized for datasets generated by the PRISM platform, any high-dimensional cell-fitness dataset can be used.","title":"3. Use the Curie tool to query viability data using cell-sets"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 What is a SigTool? \u00b6 A SigTool is a command-line program that allows easy execution of core CMap algorithms and methods via a standardized user interface. What are the features / benefits of a SigTool? \u00b6 Sig Tools provide a reference implementation of core CMap algorithms and facilitate reproducibile analytic workflows and sharing of methods. They also enable easy deployment of tools to production and cloud compute environments. The tools are designed to share common conventions and offer a unified interface such as argument parsing, input validation and unit-tests. What SigTools are available? \u00b6 The Command Reference enumerates the currently available tools organized by function. Can I run SigTools w/o configuring a custom analysis environment or access to to Matlab? \u00b6 We provide Docker images of the sig-tools at DockerHub. These images come pre-configured with all the software dependencies of the tool execute without requiring commercial licences. See here for a tutorial on how to run custom analyses using Dockerized SigTools. Where can I find the source code for sig-tools? \u00b6 The source code for all Matlab based SigTools is available in the cmapM Github repository. How should should I cite these tools in my research? \u00b6 You can use the following citation: Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. Cell 171, 1437\u20131452.e17 (2017)","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq/#what-is-a-sigtool","text":"A SigTool is a command-line program that allows easy execution of core CMap algorithms and methods via a standardized user interface.","title":"What is a SigTool?"},{"location":"faq/#what-are-the-features-benefits-of-a-sigtool","text":"Sig Tools provide a reference implementation of core CMap algorithms and facilitate reproducibile analytic workflows and sharing of methods. They also enable easy deployment of tools to production and cloud compute environments. The tools are designed to share common conventions and offer a unified interface such as argument parsing, input validation and unit-tests.","title":"What are the features / benefits of a SigTool?"},{"location":"faq/#what-sigtools-are-available","text":"The Command Reference enumerates the currently available tools organized by function.","title":"What SigTools are available?"},{"location":"faq/#can-i-run-sigtools-wo-configuring-a-custom-analysis-environment-or-access-to-to-matlab","text":"We provide Docker images of the sig-tools at DockerHub. These images come pre-configured with all the software dependencies of the tool execute without requiring commercial licences. See here for a tutorial on how to run custom analyses using Dockerized SigTools.","title":"Can I run SigTools w/o configuring a custom analysis environment or access to to Matlab?"},{"location":"faq/#where-can-i-find-the-source-code-for-sig-tools","text":"The source code for all Matlab based SigTools is available in the cmapM Github repository.","title":"Where can I find the source code for sig-tools?"},{"location":"faq/#how-should-should-i-cite-these-tools-in-my-research","text":"You can use the following citation: Subramanian, A. et al. A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles. Cell 171, 1437\u20131452.e17 (2017)","title":"How should should I cite these tools in my research?"},{"location":"command-reference/shared_arguments/","text":"Shared Arguments \u00b6 These arguments are common to all tools Synopsis \u00b6 sig_*_tool [--help, -h] [--undef_action UNDEF_ACTION] [-o, --out OUT] [--runtests] [--rundemo] [--rpt RPT] [--create_subdir CREATE_SUBDIR] [--verbose VERBOSE] [--encode_url ENCODE_URL] [--config CONFIG] Arguments \u00b6 --help, -h : Show this help message and exit --undef_action UNDEF_ACTION : Action to take if an undefined argument is encountered. Default is error. Options are {error|warn|ignore} -o , --out OUT : Output path --runtests : Execute functional and unit tests. Default is 0 --rundemo : Run the tool with sample inputs. Default is 0 --rpt RPT : Report folder prefix --create_subdir CREATE_SUBDIR : Create subfolder within out for saving results. Default is 1 --verbose VERBOSE : Print debugging messages. Default is 1 --encode_url ENCODE_URL : Encode all input URLs. Default is 0 --config CONFIG : Argument configuration file","title":"Shared arguments"},{"location":"command-reference/shared_arguments/#shared-arguments","text":"These arguments are common to all tools","title":"Shared Arguments"},{"location":"command-reference/shared_arguments/#synopsis","text":"sig_*_tool [--help, -h] [--undef_action UNDEF_ACTION] [-o, --out OUT] [--runtests] [--rundemo] [--rpt RPT] [--create_subdir CREATE_SUBDIR] [--verbose VERBOSE] [--encode_url ENCODE_URL] [--config CONFIG]","title":"Synopsis"},{"location":"command-reference/shared_arguments/#arguments","text":"--help, -h : Show this help message and exit --undef_action UNDEF_ACTION : Action to take if an undefined argument is encountered. Default is error. Options are {error|warn|ignore} -o , --out OUT : Output path --runtests : Execute functional and unit tests. Default is 0 --rundemo : Run the tool with sample inputs. Default is 0 --rpt RPT : Report folder prefix --create_subdir CREATE_SUBDIR : Create subfolder within out for saving results. Default is 1 --verbose VERBOSE : Print debugging messages. Default is 1 --encode_url ENCODE_URL : Encode all input URLs. Default is 0 --config CONFIG : Argument configuration file","title":"Arguments"},{"location":"command-reference/sig_annotate_tool/","text":"sig_annotate_tool \u00b6 Read or modify metadata for a dataset Synopsis \u00b6 sig_annotate_tool [--ds DS] [--row_meta ROW_META] [--column_meta COLUMN_META] [--strip_matrix STRIP_MATRIX] Arguments \u00b6 --ds DS : Dataset in GCT or GCTX format --row_meta ROW_META : Row annotations as a TSV file. The first column or a column named id should match the row ids in the dataset --column_meta COLUMN_META : Column annotations as a TSV file. The first column or a column named id should match the column ids in the dataset --strip_matrix STRIP_MATRIX : If selected will output the original matrix with the specified annotations removed.. Default is none. Options are {row|column|both|none} Description \u00b6 This tool extracts annotations of rows and columns from a dataset. Example \u00b6 Extract row and column annotations from a dataset sig_annotate_tool --ds 'data.gctx' Update row and column annotations sig_annotate_tool --ds 'data.gctx' --row_meta 'row_meta.tsv' --column_meta 'column_meta.tsv' Remove matrix column annotations sig_annotate_tool --ds 'data.gctx' --strip_matrix 'column'","title":"annotate"},{"location":"command-reference/sig_annotate_tool/#sig_annotate_tool","text":"Read or modify metadata for a dataset","title":"sig_annotate_tool"},{"location":"command-reference/sig_annotate_tool/#synopsis","text":"sig_annotate_tool [--ds DS] [--row_meta ROW_META] [--column_meta COLUMN_META] [--strip_matrix STRIP_MATRIX]","title":"Synopsis"},{"location":"command-reference/sig_annotate_tool/#arguments","text":"--ds DS : Dataset in GCT or GCTX format --row_meta ROW_META : Row annotations as a TSV file. The first column or a column named id should match the row ids in the dataset --column_meta COLUMN_META : Column annotations as a TSV file. The first column or a column named id should match the column ids in the dataset --strip_matrix STRIP_MATRIX : If selected will output the original matrix with the specified annotations removed.. Default is none. Options are {row|column|both|none}","title":"Arguments"},{"location":"command-reference/sig_annotate_tool/#description","text":"This tool extracts annotations of rows and columns from a dataset.","title":"Description"},{"location":"command-reference/sig_annotate_tool/#example","text":"Extract row and column annotations from a dataset sig_annotate_tool --ds 'data.gctx' Update row and column annotations sig_annotate_tool --ds 'data.gctx' --row_meta 'row_meta.tsv' --column_meta 'column_meta.tsv' Remove matrix column annotations sig_annotate_tool --ds 'data.gctx' --strip_matrix 'column'","title":"Example"},{"location":"command-reference/sig_collate_tool/","text":"sig_collate_tool \u00b6 Merge datasets Synopsis \u00b6 sig_collate_tool [--files FILES] [--folders FOLDERS] [--file_wildcard FILE_WILDCARD] [--parent_folder PARENT_FOLDER] [--sub_folder SUB_FOLDER] [--row_space ROW_SPACE] [--rid RID] [--cid CID] [--exclude_rid EXCLUDE_RID] [--exclude_cid EXCLUDE_CID] [--use_gctx USE_GCTX] [--use_compression USE_COMPRESSION] [--block_size BLOCK_SIZE] [--merge_partial MERGE_PARTIAL] [--missing_value MISSING_VALUE] Arguments \u00b6 --help, -h : Show this help message and exit --help_markdown : Print help in Markdown format --undef_action UNDEF_ACTION : Action to take if an undefined argument is encountered. Default is error. Options are {error|warn|ignore} -o, --out OUT : Output path --runtests : Execute functional and unit tests. Default is 0 --rundemo : Run the tool with sample inputs. Default is 0 --rpt RPT : Report folder prefix --create_subdir CREATE_SUBDIR : Create subfolder within out for saving results. Default is 1 --verbose VERBOSE : Print debugging messages. Default is 1 --encode_url ENCODE_URL : Encode all input URLs. Default is 0 --config CONFIG : Argument configuration file --files FILES : List of files as a GRP file or cell array --folders FOLDERS : List of parent folders as a GRP file or cell array. --file_wildcard FILE_WILDCARD : Wildcard --parent_folder PARENT_FOLDER : Parent folder containing files or folders --sub_folder SUB_FOLDER : Sub folder, relative to the parent folder that contains the target file --row_space ROW_SPACE : Filter features or rows to a pre-defined space. Options are {|lm|bing|full|custom} --rid RID : List of row ids to include as GRP file or cell array. The list of ids are excluded if exclude_rid is true --cid CID : List of column ids to include as GRP file or cell array. The list of ids are excluded if exclude_cid is true --exclude_rid EXCLUDE_RID : Exclude features or rows specified by rid or row_space if true. Default is 0 --exclude_cid EXCLUDE_CID : Exclude columns specified by cid or column_space if true. Default is 0 --use_gctx USE_GCTX : Save results in GCTX format if true or GCT otherwise. Default is 1 --use_compression USE_COMPRESSION : Use compression when saving in GCTX format. Default is 1 --block_size BLOCK_SIZE : Number of files to read before writing output to disk. Default is 25 --merge_partial MERGE_PARTIAL : Merge datasets with partially overlaping ids. Default is 0. Options are {1,0} --missing_value MISSING_VALUE : Number of files to read before writing output to disk. Default is nan Description \u00b6 This tool merges a list of datasets. Examples \u00b6 Merge a list of files sig_collate_tool --files 'dslist.grp' --row_space lm Merge datasets from a list of folders sig_collate_tool --folders 'folders.grp' --cid 'columns.grp' --row_space 'lm' Merge files names score_n*.gctx from a subfolder zs/ within a list of folders sig_collate_tool --folders 'folders.grp' --file_wildcard 'score_n*.gctx' --sub_path 'zs/'","title":"collate"},{"location":"command-reference/sig_collate_tool/#sig_collate_tool","text":"Merge datasets","title":"sig_collate_tool"},{"location":"command-reference/sig_collate_tool/#synopsis","text":"sig_collate_tool [--files FILES] [--folders FOLDERS] [--file_wildcard FILE_WILDCARD] [--parent_folder PARENT_FOLDER] [--sub_folder SUB_FOLDER] [--row_space ROW_SPACE] [--rid RID] [--cid CID] [--exclude_rid EXCLUDE_RID] [--exclude_cid EXCLUDE_CID] [--use_gctx USE_GCTX] [--use_compression USE_COMPRESSION] [--block_size BLOCK_SIZE] [--merge_partial MERGE_PARTIAL] [--missing_value MISSING_VALUE]","title":"Synopsis"},{"location":"command-reference/sig_collate_tool/#arguments","text":"--help, -h : Show this help message and exit --help_markdown : Print help in Markdown format --undef_action UNDEF_ACTION : Action to take if an undefined argument is encountered. Default is error. Options are {error|warn|ignore} -o, --out OUT : Output path --runtests : Execute functional and unit tests. Default is 0 --rundemo : Run the tool with sample inputs. Default is 0 --rpt RPT : Report folder prefix --create_subdir CREATE_SUBDIR : Create subfolder within out for saving results. Default is 1 --verbose VERBOSE : Print debugging messages. Default is 1 --encode_url ENCODE_URL : Encode all input URLs. Default is 0 --config CONFIG : Argument configuration file --files FILES : List of files as a GRP file or cell array --folders FOLDERS : List of parent folders as a GRP file or cell array. --file_wildcard FILE_WILDCARD : Wildcard --parent_folder PARENT_FOLDER : Parent folder containing files or folders --sub_folder SUB_FOLDER : Sub folder, relative to the parent folder that contains the target file --row_space ROW_SPACE : Filter features or rows to a pre-defined space. Options are {|lm|bing|full|custom} --rid RID : List of row ids to include as GRP file or cell array. The list of ids are excluded if exclude_rid is true --cid CID : List of column ids to include as GRP file or cell array. The list of ids are excluded if exclude_cid is true --exclude_rid EXCLUDE_RID : Exclude features or rows specified by rid or row_space if true. Default is 0 --exclude_cid EXCLUDE_CID : Exclude columns specified by cid or column_space if true. Default is 0 --use_gctx USE_GCTX : Save results in GCTX format if true or GCT otherwise. Default is 1 --use_compression USE_COMPRESSION : Use compression when saving in GCTX format. Default is 1 --block_size BLOCK_SIZE : Number of files to read before writing output to disk. Default is 25 --merge_partial MERGE_PARTIAL : Merge datasets with partially overlaping ids. Default is 0. Options are {1,0} --missing_value MISSING_VALUE : Number of files to read before writing output to disk. Default is nan","title":"Arguments"},{"location":"command-reference/sig_collate_tool/#description","text":"This tool merges a list of datasets.","title":"Description"},{"location":"command-reference/sig_collate_tool/#examples","text":"Merge a list of files sig_collate_tool --files 'dslist.grp' --row_space lm Merge datasets from a list of folders sig_collate_tool --folders 'folders.grp' --cid 'columns.grp' --row_space 'lm' Merge files names score_n*.gctx from a subfolder zs/ within a list of folders sig_collate_tool --folders 'folders.grp' --file_wildcard 'score_n*.gctx' --sub_path 'zs/'","title":"Examples"},{"location":"command-reference/sig_curie_tool/","text":"sig_curie_tool \u00b6 Compute similarity of input cell sets to cell viability datasets Synopsis \u00b6 sig_curie_tool [--score SCORE] [--rank RANK] [--up UP] [--down DOWN] [--min_set_size MIN_SET_SIZE] [--platform PLATFORM] [--feature_space FEATURE_SPACE] [--metric METRIC] [--es_tail ES_TAIL] [--sig_meta SIG_META] [--query_meta QUERY_META] [--skip_key_as_text SKIP_KEY_AS_TEXT] [--dataset DATASET] [--use_gctx USE_GCTX] Arguments \u00b6 --score SCORE : Dataset of differential viability scores --rank RANK : Dataset of ranks corresponding to the score dataset --up UP : Set(s) of UP cell lines --down DOWN : Set(s) of DOWN cell lines --min_set_size MIN_SET_SIZE : Minimum query set size. Sets with fewer members will be excluded. Default is 3 --platform PLATFORM : Profiling platform of the. Default is pr500_cs5. Options are {pr500_cs5} --feature_space FEATURE_SPACE : Feature identifiers used in the query cell sets. Supported options are cell_iname = CMap cell name (MCF7), ccle_name = Broad CCLE cell line name (MCF7_BREAST), feature_id = CMap feature id (c-438), cell_id = Arxspan identifiers (ACH-000019) . Default is cell_iname. Options are {feature_id|cell_iname|cell_id|ccle_name} --metric METRIC : Similarity metric. Default is wtcs. Options are {wtcs|cs} --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {both|up|down} --sig_meta SIG_META : Optional metadata for columns (signatures) of the score matrix. If provided the rows in the output datasets will be annotated using the first field as the key. --query_meta QUERY_META : Optional metadata for query cell sets. If provided the columns of the output datasets will be annotated using the first field as the key. --skip_key_as_text SKIP_KEY_AS_TEXT : If true overrides default behavior of outputting a text version of the key mnatrices. Default is 0 --dataset DATASET : Pre-canned datasets that can be queried without specifying score and rank matrices individually. Default is pasg_pr500_bydose. Options are {custom|pasg_pr500_bydose} --use_gctx USE_GCTX : Use binary GCTX format optimized for large datasets if true. Default is 0 Description \u00b6 The tool computes a set-based enrichment similarity between input cell-line sets (aka queries) and a perturbational cell-fitness signature dataset. While the tool is optimized for datasets generated by the PRISM platform, any cell-fitness dataset can be used. Examples \u00b6 Run query on a custom dataset using single-sided queries with CCLE names sig_curie_tool --score 'score.gct' --rank 'rank.gct' --up 'up.gmt' --down 'down.gmt' '--feature_space' ccle_name Run query on a pre-canned dataset using single sided queries with cell_inames as features sig_curie_tool --dataset 'pasg_pr500_bydose' --up 'up.gmt' --es_tail up","title":"curie"},{"location":"command-reference/sig_curie_tool/#sig_curie_tool","text":"Compute similarity of input cell sets to cell viability datasets","title":"sig_curie_tool"},{"location":"command-reference/sig_curie_tool/#synopsis","text":"sig_curie_tool [--score SCORE] [--rank RANK] [--up UP] [--down DOWN] [--min_set_size MIN_SET_SIZE] [--platform PLATFORM] [--feature_space FEATURE_SPACE] [--metric METRIC] [--es_tail ES_TAIL] [--sig_meta SIG_META] [--query_meta QUERY_META] [--skip_key_as_text SKIP_KEY_AS_TEXT] [--dataset DATASET] [--use_gctx USE_GCTX]","title":"Synopsis"},{"location":"command-reference/sig_curie_tool/#arguments","text":"--score SCORE : Dataset of differential viability scores --rank RANK : Dataset of ranks corresponding to the score dataset --up UP : Set(s) of UP cell lines --down DOWN : Set(s) of DOWN cell lines --min_set_size MIN_SET_SIZE : Minimum query set size. Sets with fewer members will be excluded. Default is 3 --platform PLATFORM : Profiling platform of the. Default is pr500_cs5. Options are {pr500_cs5} --feature_space FEATURE_SPACE : Feature identifiers used in the query cell sets. Supported options are cell_iname = CMap cell name (MCF7), ccle_name = Broad CCLE cell line name (MCF7_BREAST), feature_id = CMap feature id (c-438), cell_id = Arxspan identifiers (ACH-000019) . Default is cell_iname. Options are {feature_id|cell_iname|cell_id|ccle_name} --metric METRIC : Similarity metric. Default is wtcs. Options are {wtcs|cs} --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {both|up|down} --sig_meta SIG_META : Optional metadata for columns (signatures) of the score matrix. If provided the rows in the output datasets will be annotated using the first field as the key. --query_meta QUERY_META : Optional metadata for query cell sets. If provided the columns of the output datasets will be annotated using the first field as the key. --skip_key_as_text SKIP_KEY_AS_TEXT : If true overrides default behavior of outputting a text version of the key mnatrices. Default is 0 --dataset DATASET : Pre-canned datasets that can be queried without specifying score and rank matrices individually. Default is pasg_pr500_bydose. Options are {custom|pasg_pr500_bydose} --use_gctx USE_GCTX : Use binary GCTX format optimized for large datasets if true. Default is 0","title":"Arguments"},{"location":"command-reference/sig_curie_tool/#description","text":"The tool computes a set-based enrichment similarity between input cell-line sets (aka queries) and a perturbational cell-fitness signature dataset. While the tool is optimized for datasets generated by the PRISM platform, any cell-fitness dataset can be used.","title":"Description"},{"location":"command-reference/sig_curie_tool/#examples","text":"Run query on a custom dataset using single-sided queries with CCLE names sig_curie_tool --score 'score.gct' --rank 'rank.gct' --up 'up.gmt' --down 'down.gmt' '--feature_space' ccle_name Run query on a pre-canned dataset using single sided queries with cell_inames as features sig_curie_tool --dataset 'pasg_pr500_bydose' --up 'up.gmt' --es_tail up","title":"Examples"},{"location":"command-reference/sig_getgenesets_tool/","text":"sig_getgenesets_tool \u00b6 Extract top and botton N genes from dataset Synopsis: \u00b6 sig_getgenesets_tool [--ds DS] [--set_size SET_SIZE] [--cid CID] [--rid RID] [--row_space ROW_SPACE] [--sort_order SORT_ORDER] [--es_tail ES_TAIL] [--id_field ID_FIELD] [--desc_field DESC_FIELD] [--suffix SUFFIX] [--dim DIM] [--enforce_set_size ENFORCE_SET_SIZE] [--min_set_size MIN_SET_SIZE] Arguments \u00b6 --ds DS : Input dataset --set_size SET_SIZE : Integer, size of each geneset. Default is 50 --cid CID : List of column ids to extract as a GRP file or cell array. --rid RID : List of row ids to extract as a GRP file or cell array --row_space ROW_SPACE : Common row-id spaces to extract. Use '_probeset' if affy_ids present.. Default is custom. Options are {lm|lm_probeset|bing|bing_probeset|full|custom} --sort_order SORT_ORDER : String, sort order of gene expression. Default is descend. Options are {descend|ascend} --es_tail ES_TAIL : String, specifies which tail to return.. Default is both. Options are {both|up|down} --id_field ID_FIELD : String, specifies an alternate metadata field to use for selecting features instead of ids.. Default is _id --desc_field DESC_FIELD : String, specifies a metadata field to use for the output desc field. --suffix SUFFIX : String, Append string to each set name. --dim DIM : String, Dimension of matrix to operate on.. Default is column. Options are {column|row} --enforce_set_size ENFORCE_SET_SIZE : Boolean, Assert if set sizes match N exactly. A set size of less than N can result in cases where an alternate id_field is specified and there are duplicate entries. Set to false to disable this constraint.. Default is 1 --min_set_size MIN_SET_SIZE : Integer, minimum set size. Sets with fewer members are excluded from the ouput.. Default is 0 Description \u00b6 Returns the top and bottom N genesets for the score dataset DS sorted according to --sort_order. SORTORDER can be 'descend' or 'ascend'. UP and DN are structures with length equal to the number of columns in DS. Supports subsetting to specified row_space before extracting genesets with --row_space option. Outputs a GMT file. Each row in the structure has the the following fields: hd : String header, same as the column id in DS with either '_UP' or '_DN' appended. desc : String descriptor. Set to ''. entry : Cell array of N row identifiers in DS. Examples \u00b6 Get top and bottom 50 genes of each column in given dataset sig_getgenesets_tool --ds 'raw_data.gctx' --set_size 50 Get top and bottom 50 landmark genes of each column in given gene id dataset sig_getgenesets_tool --ds 'raw_data.gctx' --set_size 50 --row_space lm Get top and bottom 50 landmark genes of each column in given affy id dataset sig_getgenesets_tool --ds 'raw_data.gctx' --set_size 50 --row_space lm_probeset","title":"getgenesets"},{"location":"command-reference/sig_getgenesets_tool/#sig_getgenesets_tool","text":"Extract top and botton N genes from dataset","title":"sig_getgenesets_tool"},{"location":"command-reference/sig_getgenesets_tool/#synopsis","text":"sig_getgenesets_tool [--ds DS] [--set_size SET_SIZE] [--cid CID] [--rid RID] [--row_space ROW_SPACE] [--sort_order SORT_ORDER] [--es_tail ES_TAIL] [--id_field ID_FIELD] [--desc_field DESC_FIELD] [--suffix SUFFIX] [--dim DIM] [--enforce_set_size ENFORCE_SET_SIZE] [--min_set_size MIN_SET_SIZE]","title":"Synopsis:"},{"location":"command-reference/sig_getgenesets_tool/#arguments","text":"--ds DS : Input dataset --set_size SET_SIZE : Integer, size of each geneset. Default is 50 --cid CID : List of column ids to extract as a GRP file or cell array. --rid RID : List of row ids to extract as a GRP file or cell array --row_space ROW_SPACE : Common row-id spaces to extract. Use '_probeset' if affy_ids present.. Default is custom. Options are {lm|lm_probeset|bing|bing_probeset|full|custom} --sort_order SORT_ORDER : String, sort order of gene expression. Default is descend. Options are {descend|ascend} --es_tail ES_TAIL : String, specifies which tail to return.. Default is both. Options are {both|up|down} --id_field ID_FIELD : String, specifies an alternate metadata field to use for selecting features instead of ids.. Default is _id --desc_field DESC_FIELD : String, specifies a metadata field to use for the output desc field. --suffix SUFFIX : String, Append string to each set name. --dim DIM : String, Dimension of matrix to operate on.. Default is column. Options are {column|row} --enforce_set_size ENFORCE_SET_SIZE : Boolean, Assert if set sizes match N exactly. A set size of less than N can result in cases where an alternate id_field is specified and there are duplicate entries. Set to false to disable this constraint.. Default is 1 --min_set_size MIN_SET_SIZE : Integer, minimum set size. Sets with fewer members are excluded from the ouput.. Default is 0","title":"Arguments"},{"location":"command-reference/sig_getgenesets_tool/#description","text":"Returns the top and bottom N genesets for the score dataset DS sorted according to --sort_order. SORTORDER can be 'descend' or 'ascend'. UP and DN are structures with length equal to the number of columns in DS. Supports subsetting to specified row_space before extracting genesets with --row_space option. Outputs a GMT file. Each row in the structure has the the following fields: hd : String header, same as the column id in DS with either '_UP' or '_DN' appended. desc : String descriptor. Set to ''. entry : Cell array of N row identifiers in DS.","title":"Description"},{"location":"command-reference/sig_getgenesets_tool/#examples","text":"Get top and bottom 50 genes of each column in given dataset sig_getgenesets_tool --ds 'raw_data.gctx' --set_size 50 Get top and bottom 50 landmark genes of each column in given gene id dataset sig_getgenesets_tool --ds 'raw_data.gctx' --set_size 50 --row_space lm Get top and bottom 50 landmark genes of each column in given affy id dataset sig_getgenesets_tool --ds 'raw_data.gctx' --set_size 50 --row_space lm_probeset","title":"Examples"},{"location":"command-reference/sig_gseapreranked_tool/","text":"sig_gseapreranked_tool \u00b6 Run pre-ranked GSEA on a ranked matrix Synopsis \u00b6 sig_gseapreranked_tool [--up, --uptag UP] [--score SCORE] [--rank RANK] [--sig_meta SIG_META] [--metric METRIC] [--es_tail ES_TAIL] [--num_perm NUM_PERM] [--query_meta QUERY_META] [--save_minimal SAVE_MINIMAL] [--save_digests SAVE_DIGESTS] [--max_col MAX_COL] Arguments \u00b6 --up, --uptag UP : Geneset(s) to use for the up portion of the query --score SCORE : Custom dataset of differential expression scores (e.g. zscores) in GCT(X) format. Use in combination with rank parameter. --rank RANK : Custom dataset of ranks corresponding to the score matrix in GCT(X) format. Use in combination with score parameter. --sig_meta SIG_META : Signature metadata for each column in the score matrix. This is a required field. The first field must match the column id field in the score matrix. --metric METRIC : Similarity metric. Default is wtcs. Options are {wtcs|cs} --es_tail ES_TAIL : Specify direction of one-tailed statistic for enrichment metrics. Default is up. Options are {up|down} --num_perm NUM_PERM : Number of null permutations per unique set size. Default is 500 --query_meta QUERY_META : Metadata for each query. --save_minimal SAVE_MINIMAL : Save minimal output to optimize storage requirements. For enrichment based metrics only the combined scores are saved. Default is 1 --save_digests SAVE_DIGESTS : Save per-query digests. Default is 1 --max_col MAX_COL : Maximum number of columns of the score/rank matrices to read at a time. Default is 25000 Description \u00b6 The tool computes set-based enrichment analysis against a user-defined rank-ordered dataset. It determines whether a priori defined sets show statistically significant enrichment at either end of the ranking. Examples \u00b6 Run preRanked GSEA sig_gseapreranked_tool --up 'up.gmt' --score 'score.gctx'","title":"gseapreranked"},{"location":"command-reference/sig_gseapreranked_tool/#sig_gseapreranked_tool","text":"Run pre-ranked GSEA on a ranked matrix","title":"sig_gseapreranked_tool"},{"location":"command-reference/sig_gseapreranked_tool/#synopsis","text":"sig_gseapreranked_tool [--up, --uptag UP] [--score SCORE] [--rank RANK] [--sig_meta SIG_META] [--metric METRIC] [--es_tail ES_TAIL] [--num_perm NUM_PERM] [--query_meta QUERY_META] [--save_minimal SAVE_MINIMAL] [--save_digests SAVE_DIGESTS] [--max_col MAX_COL]","title":"Synopsis"},{"location":"command-reference/sig_gseapreranked_tool/#arguments","text":"--up, --uptag UP : Geneset(s) to use for the up portion of the query --score SCORE : Custom dataset of differential expression scores (e.g. zscores) in GCT(X) format. Use in combination with rank parameter. --rank RANK : Custom dataset of ranks corresponding to the score matrix in GCT(X) format. Use in combination with score parameter. --sig_meta SIG_META : Signature metadata for each column in the score matrix. This is a required field. The first field must match the column id field in the score matrix. --metric METRIC : Similarity metric. Default is wtcs. Options are {wtcs|cs} --es_tail ES_TAIL : Specify direction of one-tailed statistic for enrichment metrics. Default is up. Options are {up|down} --num_perm NUM_PERM : Number of null permutations per unique set size. Default is 500 --query_meta QUERY_META : Metadata for each query. --save_minimal SAVE_MINIMAL : Save minimal output to optimize storage requirements. For enrichment based metrics only the combined scores are saved. Default is 1 --save_digests SAVE_DIGESTS : Save per-query digests. Default is 1 --max_col MAX_COL : Maximum number of columns of the score/rank matrices to read at a time. Default is 25000","title":"Arguments"},{"location":"command-reference/sig_gseapreranked_tool/#description","text":"The tool computes set-based enrichment analysis against a user-defined rank-ordered dataset. It determines whether a priori defined sets show statistically significant enrichment at either end of the ranking.","title":"Description"},{"location":"command-reference/sig_gseapreranked_tool/#examples","text":"Run preRanked GSEA sig_gseapreranked_tool --up 'up.gmt' --score 'score.gctx'","title":"Examples"},{"location":"command-reference/sig_gutc_tool/","text":"sig_gutc_tool \u00b6 Compute similarity of input queries to CMap perturbagens, adjusting the results w.r.t to a background distribution Synopsis \u00b6 sig_gutc_tool [--query_result QUERY_RESULT] [--up, --uptag UP] [--down, --dntag DOWN] [--query_meta QUERY_META] [--is_matched IS_MATCHED] [--match_group MATCH_GROUP] [--metric METRIC] [--es_tail ES_TAIL] [--score SCORE] [--rank RANK] [--build_id BUILD_ID] [--feature_space FEATURE_SPACE] [--sample_space SAMPLE_SPACE] [--pcl_set PCL_SET] [--bkg_path BKG_PATH] [--save_matrices SAVE_MATRICES] [--save_digests SAVE_DIGESTS] Arguments \u00b6 --query_result QUERY_RESULT : Load pre-computed query results from supplied connectivity matrix. --up, --uptag UP : Geneset(s) to use for the up portion of the query --down, --dntag DOWN : Geneset(s) to use for the down portion of the query --query_meta QUERY_META : Metadata for each query. This is required for matched_mode. The following fields are required for matching with default parameters: [pert_id, cell_id, pert_idose, pert_itime] --is_matched IS_MATCHED : If true, compute GUTC in cell-line matched mode. Default is 0 --match_group MATCH_GROUP : Query grouping variable(s) for cell-line matching. Note that the tool expects 1 query per cell-line for each unique grouping. Default is pert_id|pert_idose|pert_itime --metric METRIC : Similarity metric. Default is wtcs. Options are {wtcs} --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {both|up|down} --score SCORE : Custom dataset of differential expression scores (e.g. zscores) in GCT(X) format. Use in combination with rank parameter. --rank RANK : Custom dataset of ranks corresponding to the score matrix in GCT(X) format. Use in combination with score parameter. Note that if --build_id BUILD_ID : Data build identifier. a2 refers to the GSE92742 dataset with Affymetrix feature ids. a2geneid is the same dataset mapped to Entrez GeneIDs. Default is a2geneid. Options are {a2|a2geneid} --feature_space FEATURE_SPACE : Feature space for query comparisions. Select lm for landmark space, bing for best-inferred gene space or full for complete genespace. Default is bing. Options are {lm|bing|full} --sample_space SAMPLE_SPACE : Signature space. Default is full. Options are {full} --pcl_set PCL_SET : Perturbational classes in GMT format. Default is /cmap/data/vdb/touchstone_v1.1/matched/annot/pcl_n171_20170201.gmt --bkg_path BKG_PATH : Path to background signature definition and percentile transforms. Default is /cmap/data/vdb/touchstone_v1.1/matched --save_matrices SAVE_MATRICES : Save result matrices. Default is 1 --save_digests SAVE_DIGESTS : Save per-query digest folders. Default is 1 Description \u00b6 Sig GUTC computes the similarity between input genesets (queries) and perturbational gene expression signatures in the CMap database. The results are transformed to a percentile scale and reported at different levels of granularity to aid interpretation. Briefly the algorithm operates as follows. First raw similarity scores between a query and CMap signatures are computed. While the method is agnostic to the specific similarity metric used, the default choice is a two-tailed weighted enrichment score. The raw scores are then scaled (Normalized) to adjust for co-variates like cell line and the type of perturbation. The normalized scores are transformed to percentile scores by comparing the test scores to those of a reference collection of signatures called Touchstone. The per-signature normalized connectivity scores are summarized to yield connectivity to individual perturbagens within a cell line, across-cell lines and for perturbational classes (PCLs). Any summary statistic can be employed, but in practice the maximal-quantile (MAXQ) score is used. Given a set of scores X and a pair of percentiles PL and PU, MAXQ returns the percentile value of X that has the maximum absolute value (By default GUTC uses PL=33 and PU=67). At each level of summarization, percentile scores are re-computed by comparing to the corresponding results when applied to the Touchstone signatures. For a given connection, the percentiles are computed within perturbagens with the cell type that the connection corresponds to. An important variant of GUTC is the matched mode specified by the is_matched parameter. Matched mode incorporates cell-line information when query data has been generated systematically in cell types that match the touchstone signatures. Currently this includes the following 9 cell types : [A375, A549, HEPG2, HCC515, HA1E, HT29, MCF7, PC3, VCAP]. To run GUTC in this mode, the is_matched flag should be set to true. Also, the required metadata should be provided using the query_meta argument. Note that the the tool expects 1 query per cell-line for each unique [pert_id, pert_idose, pert_itime] combination. The default query grouping variables can be changed using the match_group argument. Examples \u00b6 Run queries and apply GUTC sig_gutc_tool --up 'up.gmt' --down 'down.gmt' Apply GUTC on pre-computed query results sig_gutc_tool --query_result '/path/to/sig_query/results/wtcs.gctx' Run GUTC in cell-line matched mode sig_gutc_tool --query_result '/path/to/sig_query/results/wtcs.gctx' --query_meta '/path/to/query_info.txt' --is_matched true Run GUTC using a custom dataset, Expects that sig_gutc_tool --bkg_path '/path/to/gutc_background' --score '/path/to/modzs.gctx' --rank '/path/to/rank.gctx' --up 'up.gmt' --down 'down.gmt'","title":"gutc"},{"location":"command-reference/sig_gutc_tool/#sig_gutc_tool","text":"Compute similarity of input queries to CMap perturbagens, adjusting the results w.r.t to a background distribution","title":"sig_gutc_tool"},{"location":"command-reference/sig_gutc_tool/#synopsis","text":"sig_gutc_tool [--query_result QUERY_RESULT] [--up, --uptag UP] [--down, --dntag DOWN] [--query_meta QUERY_META] [--is_matched IS_MATCHED] [--match_group MATCH_GROUP] [--metric METRIC] [--es_tail ES_TAIL] [--score SCORE] [--rank RANK] [--build_id BUILD_ID] [--feature_space FEATURE_SPACE] [--sample_space SAMPLE_SPACE] [--pcl_set PCL_SET] [--bkg_path BKG_PATH] [--save_matrices SAVE_MATRICES] [--save_digests SAVE_DIGESTS]","title":"Synopsis"},{"location":"command-reference/sig_gutc_tool/#arguments","text":"--query_result QUERY_RESULT : Load pre-computed query results from supplied connectivity matrix. --up, --uptag UP : Geneset(s) to use for the up portion of the query --down, --dntag DOWN : Geneset(s) to use for the down portion of the query --query_meta QUERY_META : Metadata for each query. This is required for matched_mode. The following fields are required for matching with default parameters: [pert_id, cell_id, pert_idose, pert_itime] --is_matched IS_MATCHED : If true, compute GUTC in cell-line matched mode. Default is 0 --match_group MATCH_GROUP : Query grouping variable(s) for cell-line matching. Note that the tool expects 1 query per cell-line for each unique grouping. Default is pert_id|pert_idose|pert_itime --metric METRIC : Similarity metric. Default is wtcs. Options are {wtcs} --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {both|up|down} --score SCORE : Custom dataset of differential expression scores (e.g. zscores) in GCT(X) format. Use in combination with rank parameter. --rank RANK : Custom dataset of ranks corresponding to the score matrix in GCT(X) format. Use in combination with score parameter. Note that if --build_id BUILD_ID : Data build identifier. a2 refers to the GSE92742 dataset with Affymetrix feature ids. a2geneid is the same dataset mapped to Entrez GeneIDs. Default is a2geneid. Options are {a2|a2geneid} --feature_space FEATURE_SPACE : Feature space for query comparisions. Select lm for landmark space, bing for best-inferred gene space or full for complete genespace. Default is bing. Options are {lm|bing|full} --sample_space SAMPLE_SPACE : Signature space. Default is full. Options are {full} --pcl_set PCL_SET : Perturbational classes in GMT format. Default is /cmap/data/vdb/touchstone_v1.1/matched/annot/pcl_n171_20170201.gmt --bkg_path BKG_PATH : Path to background signature definition and percentile transforms. Default is /cmap/data/vdb/touchstone_v1.1/matched --save_matrices SAVE_MATRICES : Save result matrices. Default is 1 --save_digests SAVE_DIGESTS : Save per-query digest folders. Default is 1","title":"Arguments"},{"location":"command-reference/sig_gutc_tool/#description","text":"Sig GUTC computes the similarity between input genesets (queries) and perturbational gene expression signatures in the CMap database. The results are transformed to a percentile scale and reported at different levels of granularity to aid interpretation. Briefly the algorithm operates as follows. First raw similarity scores between a query and CMap signatures are computed. While the method is agnostic to the specific similarity metric used, the default choice is a two-tailed weighted enrichment score. The raw scores are then scaled (Normalized) to adjust for co-variates like cell line and the type of perturbation. The normalized scores are transformed to percentile scores by comparing the test scores to those of a reference collection of signatures called Touchstone. The per-signature normalized connectivity scores are summarized to yield connectivity to individual perturbagens within a cell line, across-cell lines and for perturbational classes (PCLs). Any summary statistic can be employed, but in practice the maximal-quantile (MAXQ) score is used. Given a set of scores X and a pair of percentiles PL and PU, MAXQ returns the percentile value of X that has the maximum absolute value (By default GUTC uses PL=33 and PU=67). At each level of summarization, percentile scores are re-computed by comparing to the corresponding results when applied to the Touchstone signatures. For a given connection, the percentiles are computed within perturbagens with the cell type that the connection corresponds to. An important variant of GUTC is the matched mode specified by the is_matched parameter. Matched mode incorporates cell-line information when query data has been generated systematically in cell types that match the touchstone signatures. Currently this includes the following 9 cell types : [A375, A549, HEPG2, HCC515, HA1E, HT29, MCF7, PC3, VCAP]. To run GUTC in this mode, the is_matched flag should be set to true. Also, the required metadata should be provided using the query_meta argument. Note that the the tool expects 1 query per cell-line for each unique [pert_id, pert_idose, pert_itime] combination. The default query grouping variables can be changed using the match_group argument.","title":"Description"},{"location":"command-reference/sig_gutc_tool/#examples","text":"Run queries and apply GUTC sig_gutc_tool --up 'up.gmt' --down 'down.gmt' Apply GUTC on pre-computed query results sig_gutc_tool --query_result '/path/to/sig_query/results/wtcs.gctx' Run GUTC in cell-line matched mode sig_gutc_tool --query_result '/path/to/sig_query/results/wtcs.gctx' --query_meta '/path/to/query_info.txt' --is_matched true Run GUTC using a custom dataset, Expects that sig_gutc_tool --bkg_path '/path/to/gutc_background' --score '/path/to/modzs.gctx' --rank '/path/to/rank.gctx' --up 'up.gmt' --down 'down.gmt'","title":"Examples"},{"location":"command-reference/sig_introspect_tool/","text":"sig_introspect_tool \u00b6 Compute internal connectivities between signatures Synopsis \u00b6 sig_introspect_tool [--sig_score SIG_SCORE] [--sig_rank SIG_RANK] [--sig_connectivity SIG_CONNECTIVITY] [--sig_col_meta SIG_COL_META] [--bkg_connectivity BKG_CONNECTIVITY] [--bkg_row_meta BKG_ROW_META] [--gset_size GSET_SIZE] [--metric METRIC] [--es_tail ES_TAIL] [--rid RID] [--row_space ROW_SPACE] [--symmetricize_result SYMMETRICIZE_RESULT] [--max_el MAX_EL] [--use_gctx USE_GCTX] Arguments \u00b6 --sig_score SIG_SCORE : Input signature scores --sig_rank SIG_RANK : Ranks of input signatures --sig_connectivity SIG_CONNECTIVITY : Pre-computed self query results. --sig_col_meta SIG_COL_META : Alternative column annotations for the signature score matrix. Must include column id, pert_type and cell_id as a minimum. The column id field must be the first column or be named id. --bkg_connectivity BKG_CONNECTIVITY : Pre-computed query results (e.g. wtcs scores) using identical genesets against the designated background signatures. Note for proper interpretation, the query metric and parameters used here must match those supplied to introspect. --bkg_row_meta BKG_ROW_META : Alternative row annotations for background connectivity matrix. Must include row id, pert_type and cell_id as a minimum. The row id field must be the first column or be named id. --gset_size GSET_SIZE : Length of genesets to use for enrichment metrics. Default is 50 --metric METRIC : Similarity metric. Default is wtcs. Options are {cs|wtcs} --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {up|down|both} --rid RID : List of row ids to to use specified as a GRP file or cell array. If empty all rows are used --row_space ROW_SPACE : Common row-id space definitions to use as an alternative to the rid parameter. Default is all. Options are {all|lm|bing|aig|lm_probeset|bing_probeset|full_probeset|custom} --symmetricize_result SYMMETRICIZE_RESULT : Make the result matrices symmetric by averaging the values with their transpose.. Default is 1. Options are {1,0} --max_el MAX_EL : Maximum number of elements to read at a time. Default is 2.5e+08 --use_gctx USE_GCTX : Save results in binary GCTX format if true, text GCT format if false. Default is 1 Description \u00b6 The introspect analysis examines the similarities between a group of gene expression signatures. It takes a matrix of signatures as input and computes similarities between, them adjusting the strength of connectivities using all the signatures or a pre-computed connectivity matrix as a background distribution. It does this by first executing queries of the signatures against one another. Different similarity metrics of similarity are supported, see sig_query_tool for a description of the metrics. Next the similarity scores are normalized i.e. adjusted for co-variates of perturbagen type and cell line identity. Finally the scores are converted to percentile scores using a user-specified signature set as a null distribution. Examples \u00b6 Run introspect using the signature matrix as the background sig_introspect_tool --sig_score 'sig_zscore.gct' Run introspect using a pre-computed background sig_introspect_tool --sig_score 'sig_zscore.gct --bkg_connectivity 'bkg_wtcs.gct' Run introspect using pre-computed introspect and background results sig_introspect_tool --sig_connectivity 'sig_wtcs.gct' --bkg_connectivity 'bkg_wtcs.gct'","title":"introspect"},{"location":"command-reference/sig_introspect_tool/#sig_introspect_tool","text":"Compute internal connectivities between signatures","title":"sig_introspect_tool"},{"location":"command-reference/sig_introspect_tool/#synopsis","text":"sig_introspect_tool [--sig_score SIG_SCORE] [--sig_rank SIG_RANK] [--sig_connectivity SIG_CONNECTIVITY] [--sig_col_meta SIG_COL_META] [--bkg_connectivity BKG_CONNECTIVITY] [--bkg_row_meta BKG_ROW_META] [--gset_size GSET_SIZE] [--metric METRIC] [--es_tail ES_TAIL] [--rid RID] [--row_space ROW_SPACE] [--symmetricize_result SYMMETRICIZE_RESULT] [--max_el MAX_EL] [--use_gctx USE_GCTX]","title":"Synopsis"},{"location":"command-reference/sig_introspect_tool/#arguments","text":"--sig_score SIG_SCORE : Input signature scores --sig_rank SIG_RANK : Ranks of input signatures --sig_connectivity SIG_CONNECTIVITY : Pre-computed self query results. --sig_col_meta SIG_COL_META : Alternative column annotations for the signature score matrix. Must include column id, pert_type and cell_id as a minimum. The column id field must be the first column or be named id. --bkg_connectivity BKG_CONNECTIVITY : Pre-computed query results (e.g. wtcs scores) using identical genesets against the designated background signatures. Note for proper interpretation, the query metric and parameters used here must match those supplied to introspect. --bkg_row_meta BKG_ROW_META : Alternative row annotations for background connectivity matrix. Must include row id, pert_type and cell_id as a minimum. The row id field must be the first column or be named id. --gset_size GSET_SIZE : Length of genesets to use for enrichment metrics. Default is 50 --metric METRIC : Similarity metric. Default is wtcs. Options are {cs|wtcs} --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {up|down|both} --rid RID : List of row ids to to use specified as a GRP file or cell array. If empty all rows are used --row_space ROW_SPACE : Common row-id space definitions to use as an alternative to the rid parameter. Default is all. Options are {all|lm|bing|aig|lm_probeset|bing_probeset|full_probeset|custom} --symmetricize_result SYMMETRICIZE_RESULT : Make the result matrices symmetric by averaging the values with their transpose.. Default is 1. Options are {1,0} --max_el MAX_EL : Maximum number of elements to read at a time. Default is 2.5e+08 --use_gctx USE_GCTX : Save results in binary GCTX format if true, text GCT format if false. Default is 1","title":"Arguments"},{"location":"command-reference/sig_introspect_tool/#description","text":"The introspect analysis examines the similarities between a group of gene expression signatures. It takes a matrix of signatures as input and computes similarities between, them adjusting the strength of connectivities using all the signatures or a pre-computed connectivity matrix as a background distribution. It does this by first executing queries of the signatures against one another. Different similarity metrics of similarity are supported, see sig_query_tool for a description of the metrics. Next the similarity scores are normalized i.e. adjusted for co-variates of perturbagen type and cell line identity. Finally the scores are converted to percentile scores using a user-specified signature set as a null distribution.","title":"Description"},{"location":"command-reference/sig_introspect_tool/#examples","text":"Run introspect using the signature matrix as the background sig_introspect_tool --sig_score 'sig_zscore.gct' Run introspect using a pre-computed background sig_introspect_tool --sig_score 'sig_zscore.gct --bkg_connectivity 'bkg_wtcs.gct' Run introspect using pre-computed introspect and background results sig_introspect_tool --sig_connectivity 'sig_wtcs.gct' --bkg_connectivity 'bkg_wtcs.gct'","title":"Examples"},{"location":"command-reference/sig_marker_tool/","text":"sig_marker_tool \u00b6 Identify differentially expressed genes using two-class marker selection. Synopsis \u00b6 sig_marker_tool [--ds DS] [--col_meta COL_META] [--row_meta ROW_META] [--phenotype PHENOTYPE] [--metric METRIC] [--feature_space FEATURE_SPACE] [--feature_id FEATURE_ID] [--ignore_missing_features IGNORE_MISSING_FEATURES] [--islog2 ISLOG2] [--nmarker NMARKER] [--fix_low_var FIX_LOW_VAR] [--min_sample_size MIN_SAMPLE_SIZE] [--use_gctx USE_GCTX] [--skip_rpt SKIP_RPT] [--add_heatmap_fields ADD_HEATMAP_FIELDS] Arguments \u00b6 --ds DS : Dataset of gene expression profiles [GCT or GCTX] --col_meta COL_META : Optional column metadata table --row_meta ROW_META : Optional row metadata table --phenotype PHENOTYPE : Phenotype class definition file [TSV]. It should include sample_id, and class_id and sig_id fields and optionally class_label. The sample_id field corresponds to column identifiers in the supplied dataset. The following convention is preferred class_id = A denotes the treatment class or class of interest and class_id = B is the control class. The sig_id field specifies the name of the output signature. Here is an example: sample_id class_id class_label sig_id s1 A Estradiol treatment Estradiol vs DMSO treatment s2 A Estradiol treatment Estradiol vs DMSO treatment s3 A Estradiol treatment Estradiol vs DMSO treatment s4 B DMSO treatment Estradiol vs DMSO treatment s4 B DMSO treatment Estradiol vs DMSO treatment s6 B DMSO treatment Estradiol vs DMSO treatment --metric METRIC : Test statistic to use when computing differential expression. The signal to noise metric (S2N) is used by default and is defined as: S2N = (mean of class A - mean of class B) / (stdev of A + stdev of B) The following adjustments for low variance are applied when computing the class standard deviations: When the number of samples in a class is < 10, then min_stdev = Max(0.025*class_mean, 0.025) class_stdev = Max(class_stdev, min_stdev) else The class standard deviation should be at least 0.025 If the metric is 's2n_robust' median and MAD are used in place of the class mean and stdev in the formula above. If the metric is 'fc' the fold-change is computed as follows: FC = mean of class A - mean of class B . Default is s2n. Options are {s2n|s2n_robust|fc} --feature_space FEATURE_SPACE : Subset to a predefined feature space before performing marker selection. If 'all' is specified, all features are used without subsetting. Default is all. Options are {all|lm|bing|aig|lm_probeset|bing_probeset|full_probeset} --feature_id FEATURE_ID : Custom feature space to use for marker selection, specified as a GRP file of feature ids. This flag overrides feature_space. --ignore_missing_features IGNORE_MISSING_FEATURES : Will ignore missing features if true. Default is 0 --islog2 ISLOG2 : Specify if the data is log2 transformed. Default is 1 --nmarker NMARKER : Number of markers to select. Default is 100 --fix_low_var FIX_LOW_VAR : Adjust for low variance genes. Default is 1 --min_sample_size MIN_SAMPLE_SIZE : Minimum number of samples per class. Default is 3 --use_gctx USE_GCTX : Output results as binary GCTX files. If false output as text GCT files instead. Default is 1 --skip_rpt SKIP_RPT : Skip creation of individual folders for each sig_id. Saves time. Default is 0 --add_heatmap_fields ADD_HEATMAP_FIELDS : Additional row metadata fields to include in the generated heatmap Description \u00b6 sig_marker_tool compares the expression profiles of two predetermined classes, computes differential expression scores and selects the most differentially expressed features (markers). Note data is assumed to be in log2 scale. For data in natural scale set the --islog2 flag to transform the data before computing the scores. Examples \u00b6 Run marker selection on the expression datatset EXP_FILE for the classes specified in CLASS_FILE sig_marker_tool('--ds', EXP_FILE, '--phenotype', CLASS_FILE)","title":"marker"},{"location":"command-reference/sig_marker_tool/#sig_marker_tool","text":"Identify differentially expressed genes using two-class marker selection.","title":"sig_marker_tool"},{"location":"command-reference/sig_marker_tool/#synopsis","text":"sig_marker_tool [--ds DS] [--col_meta COL_META] [--row_meta ROW_META] [--phenotype PHENOTYPE] [--metric METRIC] [--feature_space FEATURE_SPACE] [--feature_id FEATURE_ID] [--ignore_missing_features IGNORE_MISSING_FEATURES] [--islog2 ISLOG2] [--nmarker NMARKER] [--fix_low_var FIX_LOW_VAR] [--min_sample_size MIN_SAMPLE_SIZE] [--use_gctx USE_GCTX] [--skip_rpt SKIP_RPT] [--add_heatmap_fields ADD_HEATMAP_FIELDS]","title":"Synopsis"},{"location":"command-reference/sig_marker_tool/#arguments","text":"--ds DS : Dataset of gene expression profiles [GCT or GCTX] --col_meta COL_META : Optional column metadata table --row_meta ROW_META : Optional row metadata table --phenotype PHENOTYPE : Phenotype class definition file [TSV]. It should include sample_id, and class_id and sig_id fields and optionally class_label. The sample_id field corresponds to column identifiers in the supplied dataset. The following convention is preferred class_id = A denotes the treatment class or class of interest and class_id = B is the control class. The sig_id field specifies the name of the output signature. Here is an example: sample_id class_id class_label sig_id s1 A Estradiol treatment Estradiol vs DMSO treatment s2 A Estradiol treatment Estradiol vs DMSO treatment s3 A Estradiol treatment Estradiol vs DMSO treatment s4 B DMSO treatment Estradiol vs DMSO treatment s4 B DMSO treatment Estradiol vs DMSO treatment s6 B DMSO treatment Estradiol vs DMSO treatment --metric METRIC : Test statistic to use when computing differential expression. The signal to noise metric (S2N) is used by default and is defined as: S2N = (mean of class A - mean of class B) / (stdev of A + stdev of B) The following adjustments for low variance are applied when computing the class standard deviations: When the number of samples in a class is < 10, then min_stdev = Max(0.025*class_mean, 0.025) class_stdev = Max(class_stdev, min_stdev) else The class standard deviation should be at least 0.025 If the metric is 's2n_robust' median and MAD are used in place of the class mean and stdev in the formula above. If the metric is 'fc' the fold-change is computed as follows: FC = mean of class A - mean of class B . Default is s2n. Options are {s2n|s2n_robust|fc} --feature_space FEATURE_SPACE : Subset to a predefined feature space before performing marker selection. If 'all' is specified, all features are used without subsetting. Default is all. Options are {all|lm|bing|aig|lm_probeset|bing_probeset|full_probeset} --feature_id FEATURE_ID : Custom feature space to use for marker selection, specified as a GRP file of feature ids. This flag overrides feature_space. --ignore_missing_features IGNORE_MISSING_FEATURES : Will ignore missing features if true. Default is 0 --islog2 ISLOG2 : Specify if the data is log2 transformed. Default is 1 --nmarker NMARKER : Number of markers to select. Default is 100 --fix_low_var FIX_LOW_VAR : Adjust for low variance genes. Default is 1 --min_sample_size MIN_SAMPLE_SIZE : Minimum number of samples per class. Default is 3 --use_gctx USE_GCTX : Output results as binary GCTX files. If false output as text GCT files instead. Default is 1 --skip_rpt SKIP_RPT : Skip creation of individual folders for each sig_id. Saves time. Default is 0 --add_heatmap_fields ADD_HEATMAP_FIELDS : Additional row metadata fields to include in the generated heatmap","title":"Arguments"},{"location":"command-reference/sig_marker_tool/#description","text":"sig_marker_tool compares the expression profiles of two predetermined classes, computes differential expression scores and selects the most differentially expressed features (markers). Note data is assumed to be in log2 scale. For data in natural scale set the --islog2 flag to transform the data before computing the scores.","title":"Description"},{"location":"command-reference/sig_marker_tool/#examples","text":"Run marker selection on the expression datatset EXP_FILE for the classes specified in CLASS_FILE sig_marker_tool('--ds', EXP_FILE, '--phenotype', CLASS_FILE)","title":"Examples"},{"location":"command-reference/sig_pca_tool/","text":"sig_pca_tool \u00b6 Perform Principal Components Analysis on a dataset Synopsis \u00b6 sig_pca_tool [--ds DS] [--ds_meta DS_META] [--sample_dim SAMPLE_DIM] [--cid CID] [--rid RID] [--row_space ROW_SPACE] [--disable_table DISABLE_TABLE] Arguments \u00b6 --ds DS : Input dataset --ds_meta DS_META : Optional annotations as a TSV table for the input dataset for the dimension being operated on. The first column must match the corresponding id field in ds --sample_dim SAMPLE_DIM : Dimension of the dataset corresponding to samples or observations. Default is column. Options are {1|2|column|row} --cid CID : List of column ids to use specified as a GRP file or cell array. If empty all columns are used. --rid RID : List of row ids to to use specified as a GRP file or cell array. If empty all rows are used --row_space ROW_SPACE : Common row-id space definitions to use as an alternative to the rid parameter. Default is all. Options are {all|lm|bing|aig|lm_probeset|bing_probeset|full_probeset|custom} --disable_table DISABLE_TABLE : Disable generating annotated text table for first two components. The table can be generated post-hoc from the saved pc_score matrix if needed.. Default is 0 Description \u00b6 This tool applies Principal Components Analysis (PCA) on raw data. Examples \u00b6 Apply PCA on columns of a matrix sig_pca_tool --ds 'raw_data.gctx' Merge datasets from a list of folders sig_pca_tool --folders 'folders.grp' --cid 'columns.grp' --row_space 'lm'","title":"pca"},{"location":"command-reference/sig_pca_tool/#sig_pca_tool","text":"Perform Principal Components Analysis on a dataset","title":"sig_pca_tool"},{"location":"command-reference/sig_pca_tool/#synopsis","text":"sig_pca_tool [--ds DS] [--ds_meta DS_META] [--sample_dim SAMPLE_DIM] [--cid CID] [--rid RID] [--row_space ROW_SPACE] [--disable_table DISABLE_TABLE]","title":"Synopsis"},{"location":"command-reference/sig_pca_tool/#arguments","text":"--ds DS : Input dataset --ds_meta DS_META : Optional annotations as a TSV table for the input dataset for the dimension being operated on. The first column must match the corresponding id field in ds --sample_dim SAMPLE_DIM : Dimension of the dataset corresponding to samples or observations. Default is column. Options are {1|2|column|row} --cid CID : List of column ids to use specified as a GRP file or cell array. If empty all columns are used. --rid RID : List of row ids to to use specified as a GRP file or cell array. If empty all rows are used --row_space ROW_SPACE : Common row-id space definitions to use as an alternative to the rid parameter. Default is all. Options are {all|lm|bing|aig|lm_probeset|bing_probeset|full_probeset|custom} --disable_table DISABLE_TABLE : Disable generating annotated text table for first two components. The table can be generated post-hoc from the saved pc_score matrix if needed.. Default is 0","title":"Arguments"},{"location":"command-reference/sig_pca_tool/#description","text":"This tool applies Principal Components Analysis (PCA) on raw data.","title":"Description"},{"location":"command-reference/sig_pca_tool/#examples","text":"Apply PCA on columns of a matrix sig_pca_tool --ds 'raw_data.gctx' Merge datasets from a list of folders sig_pca_tool --folders 'folders.grp' --cid 'columns.grp' --row_space 'lm'","title":"Examples"},{"location":"command-reference/sig_queryl1k_tool/","text":"sig_queryl1k_tool \u00b6 Compute similarity of input genesets to perturbational signatures Synopsis \u00b6 sig_queryl1k_tool [--up, --uptag UP] [--down, --dn, --dntag DOWN] [--score SCORE] [--rank RANK] [--sig_meta SIG_META] [--metric METRIC] [--es_tail ES_TAIL] [--query_meta QUERY_META] [--ncs_group NCS_GROUP] [--save_minimal SAVE_MINIMAL] [--save_digests SAVE_DIGESTS] [--exemplar_field EXEMPLAR_FIELD] [--max_col MAX_COL] Arguments \u00b6 --up, --uptag UP : Geneset(s) to use for the up portion of the query --down, --dn, --dntag DOWN : Geneset(s) to use for the down portion of the query --score SCORE : Custom dataset of differential expression scores (e.g. zscores) in GCT(X) format. Use in combination with rank parameter. --rank RANK : Custom dataset of ranks corresponding to the score matrix in GCT(X) format. Use in combination with score parameter. --sig_meta SIG_META : Signature metadata for each column in the score matrix. This is a required field. The first field must match the column id field in the score matrix. The following fields are required [sig_id, is_ncs_sig, is_null_sig]. In addition fields specified for ncs_group and exemplar_field arguments must be present --metric METRIC : Similarity metric. Default is wtcs. Options are {wtcs|cs} --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {both|up|down} --query_meta QUERY_META : Metadata for each query. --ncs_group NCS_GROUP : Grouping field(s) used to normalize connectivity scores --save_minimal SAVE_MINIMAL : Save minimal output to optimize storage requirements. For enrichment based metrics only the combined scores are saved. Default is 1 --save_digests SAVE_DIGESTS : Save per-query digests. Default is 1 --exemplar_field EXEMPLAR_FIELD : If defined the field should exist in the sig_meta file and have (0,1) values. The per-query digests are filtered to signatures where the value>0. Default is is_exemplar_sig --max_col MAX_COL : Maximum number of columns of the score/rank matrices to read at a time. Default is 25000 Description \u00b6 The tool computes a set-based enrichment similarity between input genesets (aka queries) and a perturbational gene-expression signature dataset. While the tool is optimized for datasets generated by the L1000 platform, any perturbational dataset can be used. The algorithm operates as follows. First raw similarity (connectivity) scores between a query and CMap signatures are computed. While query methodology is agnostic to the specific similarity metric, the default choice is a non-parametric, two-tailed weighted gene-set enrichment score. The raw scores are then scaled (normalized) by the signed-means to allow for comparisons across different queries. Finally the statistical significance of the connections adjusted for multiple hypotheses is estimated. FDR q-values are estimated by comparing the distributions of treatments to null signatures in the dataset. Outputs: \u00b6 The tool produces the following output: arfs/ : Per-query analysis report files (ARFs) /query_result.gct : a GCT format text file listing the annotations, connectivity scores and q-values for each signature in the dataset. The following fields are computed by the query tool: raw_cs : Raw connectivity scores norm_cs : Normalized connectivity score computed by dividing the raw connectivity scores by the signed-mean scores of signatures (specified by the is_ncs_sig field in the signature metadata file) If the ncs_group field is not empty the scores are normalized within each group, otherwise the scores are normalized using the global means across all signatures. fdr_q_nlog10 : Negative log10 transformed FDR q-values estimated relative to the null signatures (specified by the is_null_sig field in the signature annotation file). matrices/query : Query parameters and result matrices in GCTx format for all queries: up.gmt, dn.gmt: query genesets in GMT format cs.gctx : Raw connectivity scores matrix [signatures x queries] ncs.gctx : Normalized connectivity score matrix [signatures x queries] fdr_qvalue.gctx : Estimated false discovery rate q-values [signatures x queries] Examples \u00b6 % Run queries sig_queryl1k_tool --up 'up.gmt' --down 'down.gmt' --score 'score.gctx' --rank 'rank.gctx' --sig_meta 'sig_meta.txt'","title":"queryl1k"},{"location":"command-reference/sig_queryl1k_tool/#sig_queryl1k_tool","text":"Compute similarity of input genesets to perturbational signatures","title":"sig_queryl1k_tool"},{"location":"command-reference/sig_queryl1k_tool/#synopsis","text":"sig_queryl1k_tool [--up, --uptag UP] [--down, --dn, --dntag DOWN] [--score SCORE] [--rank RANK] [--sig_meta SIG_META] [--metric METRIC] [--es_tail ES_TAIL] [--query_meta QUERY_META] [--ncs_group NCS_GROUP] [--save_minimal SAVE_MINIMAL] [--save_digests SAVE_DIGESTS] [--exemplar_field EXEMPLAR_FIELD] [--max_col MAX_COL]","title":"Synopsis"},{"location":"command-reference/sig_queryl1k_tool/#arguments","text":"--up, --uptag UP : Geneset(s) to use for the up portion of the query --down, --dn, --dntag DOWN : Geneset(s) to use for the down portion of the query --score SCORE : Custom dataset of differential expression scores (e.g. zscores) in GCT(X) format. Use in combination with rank parameter. --rank RANK : Custom dataset of ranks corresponding to the score matrix in GCT(X) format. Use in combination with score parameter. --sig_meta SIG_META : Signature metadata for each column in the score matrix. This is a required field. The first field must match the column id field in the score matrix. The following fields are required [sig_id, is_ncs_sig, is_null_sig]. In addition fields specified for ncs_group and exemplar_field arguments must be present --metric METRIC : Similarity metric. Default is wtcs. Options are {wtcs|cs} --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {both|up|down} --query_meta QUERY_META : Metadata for each query. --ncs_group NCS_GROUP : Grouping field(s) used to normalize connectivity scores --save_minimal SAVE_MINIMAL : Save minimal output to optimize storage requirements. For enrichment based metrics only the combined scores are saved. Default is 1 --save_digests SAVE_DIGESTS : Save per-query digests. Default is 1 --exemplar_field EXEMPLAR_FIELD : If defined the field should exist in the sig_meta file and have (0,1) values. The per-query digests are filtered to signatures where the value>0. Default is is_exemplar_sig --max_col MAX_COL : Maximum number of columns of the score/rank matrices to read at a time. Default is 25000","title":"Arguments"},{"location":"command-reference/sig_queryl1k_tool/#description","text":"The tool computes a set-based enrichment similarity between input genesets (aka queries) and a perturbational gene-expression signature dataset. While the tool is optimized for datasets generated by the L1000 platform, any perturbational dataset can be used. The algorithm operates as follows. First raw similarity (connectivity) scores between a query and CMap signatures are computed. While query methodology is agnostic to the specific similarity metric, the default choice is a non-parametric, two-tailed weighted gene-set enrichment score. The raw scores are then scaled (normalized) by the signed-means to allow for comparisons across different queries. Finally the statistical significance of the connections adjusted for multiple hypotheses is estimated. FDR q-values are estimated by comparing the distributions of treatments to null signatures in the dataset.","title":"Description"},{"location":"command-reference/sig_queryl1k_tool/#outputs","text":"The tool produces the following output: arfs/ : Per-query analysis report files (ARFs) /query_result.gct : a GCT format text file listing the annotations, connectivity scores and q-values for each signature in the dataset. The following fields are computed by the query tool: raw_cs : Raw connectivity scores norm_cs : Normalized connectivity score computed by dividing the raw connectivity scores by the signed-mean scores of signatures (specified by the is_ncs_sig field in the signature metadata file) If the ncs_group field is not empty the scores are normalized within each group, otherwise the scores are normalized using the global means across all signatures. fdr_q_nlog10 : Negative log10 transformed FDR q-values estimated relative to the null signatures (specified by the is_null_sig field in the signature annotation file). matrices/query : Query parameters and result matrices in GCTx format for all queries: up.gmt, dn.gmt: query genesets in GMT format cs.gctx : Raw connectivity scores matrix [signatures x queries] ncs.gctx : Normalized connectivity score matrix [signatures x queries] fdr_qvalue.gctx : Estimated false discovery rate q-values [signatures x queries]","title":"Outputs:"},{"location":"command-reference/sig_queryl1k_tool/#examples","text":"% Run queries sig_queryl1k_tool --up 'up.gmt' --down 'down.gmt' --score 'score.gctx' --rank 'rank.gctx' --sig_meta 'sig_meta.txt'","title":"Examples"},{"location":"command-reference/sig_recall_tool/","text":"sig_recall_tool \u00b6 Compare replicates signatures to assess similarity Synopsis: \u00b6 sig_recall_tool [--ds_list DS_LIST] [--metric METRIC] [--set_size SET_SIZE] [--es_tail ES_TAIL] [--dim DIM] [--sample_field SAMPLE_FIELD] [--feature_field FEATURE_FIELD] [--row_filter ROW_FILTER] [--column_filter COLUMN_FILTER] [--save_pw_matrix SAVE_PW_MATRIX] [--recall_group_prefix RECALL_GROUP_PREFIX] [--outlier_alpha OUTLIER_ALPHA] [--fix_ties FIX_TIES] Arguments \u00b6 --ds_list DS_LIST : List of datasets to compare. A single replicate set can be specified as a GRP file or cell array listing the full filepath to a dataset per line. Multiple replicate sets can be specified by supplying a TSV text file with the following columns: group_id : Grouping variable shared by all datasets in a replicate set file_path : Full filepath to a dataset For example to run recall on two replicate sets A and B use: group_id file_path A /path/to/DS_A_X1.gct A /path/to/DS_A_X2.gctx A /path/to/DS_A_X3.gct B /path/to/DS_B_X1.gctx B /path/to/DS_B_X2.gct Any replicate set with singleton entries will be ignored. A list of skipped replicate sets is output to a file named ds_skipped.grp --metric METRIC : Similarity metric to use for the comparison. Default is spearman. Options are {spearman|pearson|wtcs|cs|cosine} --set_size SET_SIZE : Set size to use for enrichment metrics. This is ignored for correlation metrics. Default is 50 --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {up|down|both} --dim DIM : Dimension to operate on. The default is to compare columns between datasets. If 'row' is specified, the features are compared. Default is column. Options are {row|column} --sample_field SAMPLE_FIELD : Column metadata field to use for matching pairs of comparisons. The field should exist in each dataset for the dimension specified. Default is det_well --feature_field FEATURE_FIELD : Row metadata field to use for matching pairs of comparisons. The field should exist in each dataset for the dimension specified. Default is rid --row_filter ROW_FILTER : GMT or GMX file specifying row filter criteria. Dataset rows are filtered prior to recall analysis. See parse_filter for details on the filter format --column_filter COLUMN_FILTER : GMT or GMX file specifying column filter criteria. Dataset columns are filtered prior to recall analysis. See parse_filter for details on the filter format --save_pw_matrix SAVE_PW_MATRIX : If true, saves pairwise similarity matrices in GCTx format for each pair of comparisons.. Default is 0 --recall_group_prefix RECALL_GROUP_PREFIX : String if provided is prepended to the recall_group field in the recall report --outlier_alpha OUTLIER_ALPHA : Level of significance, used to flag outlier replicate datasets. Default is 0.01 --fix_ties FIX_TIES : Adjusts for ties in the recall score when computing ranks if true. Default is 1 Description \u00b6 For a grouped collection of datasets (a dataset group), the recall tool computes pairwise similarities between each pair of datasets in the group using the specified metric and dimension. In the case of non-symmetric enrichment metrics (e.g. wtcs) the similarity is assessed in both directions and averaged to ensure that the order of evaluation of matrices does not affect the result. Note that if the dimensions of the input datasets are of different sizes the pairwise similarity matrix will not be square. The recall scores are elements of the pairwise similarity matrix that correspond to matching sample_field metadata values (or feature_field if the dimension is row). Next row and column ranks are computed by ranking elements of similarity matrix both row and column-wise and converted to percentiles (ranging [0, 100] with 0 indicating perfect recall). The recall rank is computed as the average of the row and column percentile ranks for the same elements that correspond to the recall scores. The tool produces several outputs, including a summary HTML index page that lists the recall summary for each dataset group in the input. The table links to a gallery of diagnostic plots for each group. In addition the following TSV text reports are generated for each dataset group: recall_report_pairs.txt : This report provides the most granular level information of the analysis and lists the recall scores and ranks of every pair of profiles compared in addition to the corresponding metadata. The key recall fields are: recall_group : indicates the pairwise comparisions belonging to the same replicate set recall_score : similarity score of the pair of signatures. recall_rank : The average percentile rank computed from the row-wise and column-wise percentile ranks of the underlying pairwise similarity matrix. recall_composite : A combined measure ranging [0, 1] derived from the recall score and rank. Its computed as the geometric mean of clipped recall_score and recall rank as follows: recall_composite = sqrt(clip(recall_score, 0.001, inf).* (100 - recall_rank)/100) recall_report_sets.txt : A replicate set level report listing aggregate statistics for each unique recall_group derived from the metrics listed above. recall_summary.txt : A summary of recall of all replicate sets in a dataset group. In addition lists the presence and identity of outlier datasets. recall_report_datasets.txt : Recall statistics for each dataset belonging to a dataset group. In addition the recall ranks associated with each dataset are compared with each other for outliers. Examples \u00b6 Compute recall using Spearman correlation sig_recall_tool --ds_list '/list/of/datasets' --metric 'spearman' Compute recall using Two-tailed weighted Enrichment with a set size of 50 sig_recall_tool --ds_list '/list/of/datasets' --metric 'wtcs' --set_size 50","title":"recall"},{"location":"command-reference/sig_recall_tool/#sig_recall_tool","text":"Compare replicates signatures to assess similarity","title":"sig_recall_tool"},{"location":"command-reference/sig_recall_tool/#synopsis","text":"sig_recall_tool [--ds_list DS_LIST] [--metric METRIC] [--set_size SET_SIZE] [--es_tail ES_TAIL] [--dim DIM] [--sample_field SAMPLE_FIELD] [--feature_field FEATURE_FIELD] [--row_filter ROW_FILTER] [--column_filter COLUMN_FILTER] [--save_pw_matrix SAVE_PW_MATRIX] [--recall_group_prefix RECALL_GROUP_PREFIX] [--outlier_alpha OUTLIER_ALPHA] [--fix_ties FIX_TIES]","title":"Synopsis:"},{"location":"command-reference/sig_recall_tool/#arguments","text":"--ds_list DS_LIST : List of datasets to compare. A single replicate set can be specified as a GRP file or cell array listing the full filepath to a dataset per line. Multiple replicate sets can be specified by supplying a TSV text file with the following columns: group_id : Grouping variable shared by all datasets in a replicate set file_path : Full filepath to a dataset For example to run recall on two replicate sets A and B use: group_id file_path A /path/to/DS_A_X1.gct A /path/to/DS_A_X2.gctx A /path/to/DS_A_X3.gct B /path/to/DS_B_X1.gctx B /path/to/DS_B_X2.gct Any replicate set with singleton entries will be ignored. A list of skipped replicate sets is output to a file named ds_skipped.grp --metric METRIC : Similarity metric to use for the comparison. Default is spearman. Options are {spearman|pearson|wtcs|cs|cosine} --set_size SET_SIZE : Set size to use for enrichment metrics. This is ignored for correlation metrics. Default is 50 --es_tail ES_TAIL : Specify two-tailed or one-tailed statistic for enrichment metrics. Default is both. Options are {up|down|both} --dim DIM : Dimension to operate on. The default is to compare columns between datasets. If 'row' is specified, the features are compared. Default is column. Options are {row|column} --sample_field SAMPLE_FIELD : Column metadata field to use for matching pairs of comparisons. The field should exist in each dataset for the dimension specified. Default is det_well --feature_field FEATURE_FIELD : Row metadata field to use for matching pairs of comparisons. The field should exist in each dataset for the dimension specified. Default is rid --row_filter ROW_FILTER : GMT or GMX file specifying row filter criteria. Dataset rows are filtered prior to recall analysis. See parse_filter for details on the filter format --column_filter COLUMN_FILTER : GMT or GMX file specifying column filter criteria. Dataset columns are filtered prior to recall analysis. See parse_filter for details on the filter format --save_pw_matrix SAVE_PW_MATRIX : If true, saves pairwise similarity matrices in GCTx format for each pair of comparisons.. Default is 0 --recall_group_prefix RECALL_GROUP_PREFIX : String if provided is prepended to the recall_group field in the recall report --outlier_alpha OUTLIER_ALPHA : Level of significance, used to flag outlier replicate datasets. Default is 0.01 --fix_ties FIX_TIES : Adjusts for ties in the recall score when computing ranks if true. Default is 1","title":"Arguments"},{"location":"command-reference/sig_recall_tool/#description","text":"For a grouped collection of datasets (a dataset group), the recall tool computes pairwise similarities between each pair of datasets in the group using the specified metric and dimension. In the case of non-symmetric enrichment metrics (e.g. wtcs) the similarity is assessed in both directions and averaged to ensure that the order of evaluation of matrices does not affect the result. Note that if the dimensions of the input datasets are of different sizes the pairwise similarity matrix will not be square. The recall scores are elements of the pairwise similarity matrix that correspond to matching sample_field metadata values (or feature_field if the dimension is row). Next row and column ranks are computed by ranking elements of similarity matrix both row and column-wise and converted to percentiles (ranging [0, 100] with 0 indicating perfect recall). The recall rank is computed as the average of the row and column percentile ranks for the same elements that correspond to the recall scores. The tool produces several outputs, including a summary HTML index page that lists the recall summary for each dataset group in the input. The table links to a gallery of diagnostic plots for each group. In addition the following TSV text reports are generated for each dataset group: recall_report_pairs.txt : This report provides the most granular level information of the analysis and lists the recall scores and ranks of every pair of profiles compared in addition to the corresponding metadata. The key recall fields are: recall_group : indicates the pairwise comparisions belonging to the same replicate set recall_score : similarity score of the pair of signatures. recall_rank : The average percentile rank computed from the row-wise and column-wise percentile ranks of the underlying pairwise similarity matrix. recall_composite : A combined measure ranging [0, 1] derived from the recall score and rank. Its computed as the geometric mean of clipped recall_score and recall rank as follows: recall_composite = sqrt(clip(recall_score, 0.001, inf).* (100 - recall_rank)/100) recall_report_sets.txt : A replicate set level report listing aggregate statistics for each unique recall_group derived from the metrics listed above. recall_summary.txt : A summary of recall of all replicate sets in a dataset group. In addition lists the presence and identity of outlier datasets. recall_report_datasets.txt : Recall statistics for each dataset belonging to a dataset group. In addition the recall ranks associated with each dataset are compared with each other for outliers.","title":"Description"},{"location":"command-reference/sig_recall_tool/#examples","text":"Compute recall using Spearman correlation sig_recall_tool --ds_list '/list/of/datasets' --metric 'spearman' Compute recall using Two-tailed weighted Enrichment with a set size of 50 sig_recall_tool --ds_list '/list/of/datasets' --metric 'wtcs' --set_size 50","title":"Examples"},{"location":"command-reference/sig_score2rank_tool/","text":"sig_score2rank_tool \u00b6 Generate rank matrix from score matrix Synopsis \u00b6 sig_score2rank_tool [--ds DS] [--outfile OUTFILE] [--dim DIM] [--sort_order SORT_ORDER] [--ignore_nan IGNORE_NAN] [--as_fraction AS_FRACTION] [--as_percentile AS_PERCENTILE] [--fix_ties FIX_TIES] [--use_gctx USE_GCTX] [--read_mode READ_MODE] [--block_size BLOCK_SIZE] Arguments \u00b6 --ds DS : Input score matrix to compute ranks --outfile OUTFILE : Output filename of rank matrix. --dim DIM : Dimension to operate on. Default is column. Options are {column|row} --sort_order SORT_ORDER : Sort order. Default is descend. Options are {descend|ascend} --ignore_nan IGNORE_NAN : Ignore NaNs when ranking. Note that ignoring NaNs is slower. Default is 1 --as_fraction AS_FRACTION : Logical, returns ranks as a fraction of total rows. Default is 0 --as_percentile AS_PERCENTILE : Logical, returns ranks as a percentile of total rows. Default is 0 --fix_ties FIX_TIES : Logical, adjusts for ties (is false by default). Default is 0 --use_gctx USE_GCTX : Use GCTX file format. Default is 1 --read_mode READ_MODE : Specify how to process the input matrix. If full is specified the complete matrix is processed enmasse. If iterative the input matrix is processed in blocks. Default is full. Options are {full|iterative} --block_size BLOCK_SIZE : size of blocks to process if the read_mode is iterative. Default is 50000 Description \u00b6 Rank orders the input dataset along dimension dim in the order specified by sort_order . The rank matrix is used by connectivity analysis tools such as queryl1k to speed up the computation of enrichment based similarities. Examples \u00b6 Calculate ranks of input score matrix sig_score2rank_tool --ds 'raw_data.gctx'","title":"score2rank"},{"location":"command-reference/sig_score2rank_tool/#sig_score2rank_tool","text":"Generate rank matrix from score matrix","title":"sig_score2rank_tool"},{"location":"command-reference/sig_score2rank_tool/#synopsis","text":"sig_score2rank_tool [--ds DS] [--outfile OUTFILE] [--dim DIM] [--sort_order SORT_ORDER] [--ignore_nan IGNORE_NAN] [--as_fraction AS_FRACTION] [--as_percentile AS_PERCENTILE] [--fix_ties FIX_TIES] [--use_gctx USE_GCTX] [--read_mode READ_MODE] [--block_size BLOCK_SIZE]","title":"Synopsis"},{"location":"command-reference/sig_score2rank_tool/#arguments","text":"--ds DS : Input score matrix to compute ranks --outfile OUTFILE : Output filename of rank matrix. --dim DIM : Dimension to operate on. Default is column. Options are {column|row} --sort_order SORT_ORDER : Sort order. Default is descend. Options are {descend|ascend} --ignore_nan IGNORE_NAN : Ignore NaNs when ranking. Note that ignoring NaNs is slower. Default is 1 --as_fraction AS_FRACTION : Logical, returns ranks as a fraction of total rows. Default is 0 --as_percentile AS_PERCENTILE : Logical, returns ranks as a percentile of total rows. Default is 0 --fix_ties FIX_TIES : Logical, adjusts for ties (is false by default). Default is 0 --use_gctx USE_GCTX : Use GCTX file format. Default is 1 --read_mode READ_MODE : Specify how to process the input matrix. If full is specified the complete matrix is processed enmasse. If iterative the input matrix is processed in blocks. Default is full. Options are {full|iterative} --block_size BLOCK_SIZE : size of blocks to process if the read_mode is iterative. Default is 50000","title":"Arguments"},{"location":"command-reference/sig_score2rank_tool/#description","text":"Rank orders the input dataset along dimension dim in the order specified by sort_order . The rank matrix is used by connectivity analysis tools such as queryl1k to speed up the computation of enrichment based similarities.","title":"Description"},{"location":"command-reference/sig_score2rank_tool/#examples","text":"Calculate ranks of input score matrix sig_score2rank_tool --ds 'raw_data.gctx'","title":"Examples"},{"location":"command-reference/sig_slice_tool/","text":"SigSliceTool \u00b6 Extract a subset from a larger dataset Synopsis \u00b6 SigSliceTool [--ds DS] [--cid CID] [--rid RID] [--row_space ROW_SPACE] [--row_meta ROW_META] [--col_meta COL_META] [--use_gctx USE_GCTX] [--num_digits NUM_DIGITS] [--ignore_missing IGNORE_MISSING] Arguments \u00b6 --ds DS : Dataset in GCT or GCTX format --cid CID : List of column ids to extract as a GRP file or cell array. --rid RID : List of row ids to extract as a GRP file or cell array --row_space ROW_SPACE : Common row-id spaces to extract. '_probeset' refers to affy ids. Default is custom. Options are {lm|lm_probeset|bing|bing_probeset|aig|full_probeset|custom} --row_meta ROW_META : Row metadata as a TSV text file. If provided the rows in the output dataset will be annotated using the first field as the key to join with the row-ids --col_meta COL_META : Column metadata as a TSV text file. If provided the columns in the output dataset will be annotated using the first field as the key to join with the column-ids --use_gctx USE_GCTX : Save results in GCTX format if true or GCT otherwise. Default is 1 --num_digits NUM_DIGITS : Number of digits to use when writing to GCT format. Default is 4 --ignore_missing IGNORE_MISSING : If false, program will fail when missing any specified rids or cids. Default is 0 Description \u00b6 This tool extracts a subset of rows and columns from a larger dataset. Examples \u00b6 Extract a subset of columns from a larger dataset sig_slice_tool --ds 'data.gctx' --cid 'column_ids.grp' --rid 'row_ids.grp' Extract only landmark genes for a subset of columns sig_slice_tool --ds 'data.gctx' --cid 'columns.grp' --row_space lm","title":"slice"},{"location":"command-reference/sig_slice_tool/#sigslicetool","text":"Extract a subset from a larger dataset","title":"SigSliceTool"},{"location":"command-reference/sig_slice_tool/#synopsis","text":"SigSliceTool [--ds DS] [--cid CID] [--rid RID] [--row_space ROW_SPACE] [--row_meta ROW_META] [--col_meta COL_META] [--use_gctx USE_GCTX] [--num_digits NUM_DIGITS] [--ignore_missing IGNORE_MISSING]","title":"Synopsis"},{"location":"command-reference/sig_slice_tool/#arguments","text":"--ds DS : Dataset in GCT or GCTX format --cid CID : List of column ids to extract as a GRP file or cell array. --rid RID : List of row ids to extract as a GRP file or cell array --row_space ROW_SPACE : Common row-id spaces to extract. '_probeset' refers to affy ids. Default is custom. Options are {lm|lm_probeset|bing|bing_probeset|aig|full_probeset|custom} --row_meta ROW_META : Row metadata as a TSV text file. If provided the rows in the output dataset will be annotated using the first field as the key to join with the row-ids --col_meta COL_META : Column metadata as a TSV text file. If provided the columns in the output dataset will be annotated using the first field as the key to join with the column-ids --use_gctx USE_GCTX : Save results in GCTX format if true or GCT otherwise. Default is 1 --num_digits NUM_DIGITS : Number of digits to use when writing to GCT format. Default is 4 --ignore_missing IGNORE_MISSING : If false, program will fail when missing any specified rids or cids. Default is 0","title":"Arguments"},{"location":"command-reference/sig_slice_tool/#description","text":"This tool extracts a subset of rows and columns from a larger dataset.","title":"Description"},{"location":"command-reference/sig_slice_tool/#examples","text":"Extract a subset of columns from a larger dataset sig_slice_tool --ds 'data.gctx' --cid 'column_ids.grp' --rid 'row_ids.grp' Extract only landmark genes for a subset of columns sig_slice_tool --ds 'data.gctx' --cid 'columns.grp' --row_space lm","title":"Examples"},{"location":"command-reference/sig_testmlr_tool/","text":"sig_testmlr_tool \u00b6 Apply given model to predict genes using multiple linear regression. Synopsis \u00b6 sig_testmlr_tool [--ds DS] [--model MODEL] [--minval MINVAL] [--maxval MAXVAL] [--use_gctx USE_GCTX] [--xform XFORM] Arguments \u00b6 --help, -h : Show this help message and exit --help_markdown : Print help in Markdown format --undef_action UNDEF_ACTION : Action to take if an undefined argument is encountered. Default is error. Options are {error|warn|ignore} -o, --out OUT : Output path --runtests : Execute functional and unit tests. Default is 0 --rundemo : Run the tool with sample inputs. Default is 0 --rpt RPT : Report folder prefix --create_subdir CREATE_SUBDIR : Create subfolder within out for saving results. Default is 1 --verbose VERBOSE : Print debugging messages. Default is 1 --encode_url ENCODE_URL : Encode all input URLs. Default is 0 --config CONFIG : Argument configuration file --ds DS : GCT or GCTX, Input dataset (genes x samples), minimally containing all landmarks genes specified in the model. Non-landmark genes will be ignored --model MODEL : MLR model output from SIG_TRAINMLR_TOOL to utilize to infer dependent genes --minval MINVAL : Minimum value for output dataset. Lower values will be replaced with this value. Default is 0 --maxval MAXVAL : Maximum value for output dataset. Greater values will be replaced with this value. Default is 15 --use_gctx USE_GCTX : Outputs text GCT files if false. Default is 1 --xform XFORM : String. Specify any transformations to apply to data prior to training. Default is none. Options are {none|log2|abs|pow2|zscore} Description \u00b6 The sig_testmlr_tool takes as input a test dataset (genes x samples) minimally containing all landmark genes and an MLR model (output by SIG_TRAINMLR_TOOL) and output inferred values for dependent genes by applying the model. Note that the values in the input dataset must match the data used for training. The standard L1000 analytical workflow employs normalized log2 transformed expression as input to the model. The --xform argument can be optionally specified to transform the input data. The output is saved as a GCTX file with the inferred dependent genes appended to the original landmark data. Examples \u00b6 Apply model to landmark data. sig_testmlr_tool --ds 'test_data.gctx' --model '/filepath/model.gctx' Apply model to landmark data retricting the minimum value in the output to 2. sig_testmlr_tool --ds 'test_data.gctx' --model '/filepath/model.gctx' --minval 2","title":"testmlr"},{"location":"command-reference/sig_testmlr_tool/#sig_testmlr_tool","text":"Apply given model to predict genes using multiple linear regression.","title":"sig_testmlr_tool"},{"location":"command-reference/sig_testmlr_tool/#synopsis","text":"sig_testmlr_tool [--ds DS] [--model MODEL] [--minval MINVAL] [--maxval MAXVAL] [--use_gctx USE_GCTX] [--xform XFORM]","title":"Synopsis"},{"location":"command-reference/sig_testmlr_tool/#arguments","text":"--help, -h : Show this help message and exit --help_markdown : Print help in Markdown format --undef_action UNDEF_ACTION : Action to take if an undefined argument is encountered. Default is error. Options are {error|warn|ignore} -o, --out OUT : Output path --runtests : Execute functional and unit tests. Default is 0 --rundemo : Run the tool with sample inputs. Default is 0 --rpt RPT : Report folder prefix --create_subdir CREATE_SUBDIR : Create subfolder within out for saving results. Default is 1 --verbose VERBOSE : Print debugging messages. Default is 1 --encode_url ENCODE_URL : Encode all input URLs. Default is 0 --config CONFIG : Argument configuration file --ds DS : GCT or GCTX, Input dataset (genes x samples), minimally containing all landmarks genes specified in the model. Non-landmark genes will be ignored --model MODEL : MLR model output from SIG_TRAINMLR_TOOL to utilize to infer dependent genes --minval MINVAL : Minimum value for output dataset. Lower values will be replaced with this value. Default is 0 --maxval MAXVAL : Maximum value for output dataset. Greater values will be replaced with this value. Default is 15 --use_gctx USE_GCTX : Outputs text GCT files if false. Default is 1 --xform XFORM : String. Specify any transformations to apply to data prior to training. Default is none. Options are {none|log2|abs|pow2|zscore}","title":"Arguments"},{"location":"command-reference/sig_testmlr_tool/#description","text":"The sig_testmlr_tool takes as input a test dataset (genes x samples) minimally containing all landmark genes and an MLR model (output by SIG_TRAINMLR_TOOL) and output inferred values for dependent genes by applying the model. Note that the values in the input dataset must match the data used for training. The standard L1000 analytical workflow employs normalized log2 transformed expression as input to the model. The --xform argument can be optionally specified to transform the input data. The output is saved as a GCTX file with the inferred dependent genes appended to the original landmark data.","title":"Description"},{"location":"command-reference/sig_testmlr_tool/#examples","text":"Apply model to landmark data. sig_testmlr_tool --ds 'test_data.gctx' --model '/filepath/model.gctx' Apply model to landmark data retricting the minimum value in the output to 2. sig_testmlr_tool --ds 'test_data.gctx' --model '/filepath/model.gctx' --minval 2","title":"Examples"},{"location":"command-reference/sig_trainmlr_tool/","text":"sig_trainmlr_tool \u00b6 Create a model given a training dataset using multilinear regression. Synopsis: \u00b6 sig_trainmlr_tool [--ds DS] [--modeltype MODELTYPE] [--grp_landmark GRP_LANDMARK] [--dependents DEPENDENTS] [--cid CID] [--xform XFORM] [--precision PRECISION] [--outfmt OUTFMT] Arguments \u00b6 --ds DS : Input dataset that includes both landmarks and resulting outputs. --modeltype MODELTYPE : String. Regression model to be used: pinv_int - Linear Regression with an intercept. pinv - Linear Regression. All best fits lines pass through origin. . Default is pinv_int. Options are {pinv_int|pinv} --grp_landmark GRP_LANDMARK : GRP file containing identifiers for landmark genes. Must correspond to the row-ids in DS --dependents DEPENDENTS : GRP file containing identifiers for dependent genes to infer. Must correspond to row-ids in DS and be mutually exclusive from the landmarks. --cid CID : GRP file of column ids (samples) in DS to use for training. --xform XFORM : String. Specify any transformations to apply to data prior to training. Default is none. Options are {none|log2|abs|pow2|zscore} --precision PRECISION : Integer. Level of precision (decimal places) for output model weights.. Default is 6 --outfmt OUTFMT : Model Output format. Default is gctx. Options are {mat|gct|gctx} Description \u00b6 This tool takes as input a training dataset (genes x samples) containing expression values for a specified set of predictor (landmark) genes and some number of dependent genes and builds a multiple linear regression (MLR) model. The model can be applied to a new dataset using the SIG_TESTMLR_TOOL to infer the expression of the dependent genes based on landmark expression. Examples \u00b6 Create a model from training data and landmark GRP file. sig_trainmlr_tool --ds 'training_data.gctx' --grp_landmark 'landmark.grp' Create a model for a subset of dependent genes. sig_trainmlr_tool --ds 'training_data.gctx' --grp_landmark 'landmark.grp' --dependents 'subset.grp' Perform z-scoring prior to creating model. sig_trainmlr_tool --ds 'training_data.gctx' --grp_landmark 'landmark.grp' --xform 'zscore'","title":"trainmlr"},{"location":"command-reference/sig_trainmlr_tool/#sig_trainmlr_tool","text":"Create a model given a training dataset using multilinear regression.","title":"sig_trainmlr_tool"},{"location":"command-reference/sig_trainmlr_tool/#synopsis","text":"sig_trainmlr_tool [--ds DS] [--modeltype MODELTYPE] [--grp_landmark GRP_LANDMARK] [--dependents DEPENDENTS] [--cid CID] [--xform XFORM] [--precision PRECISION] [--outfmt OUTFMT]","title":"Synopsis:"},{"location":"command-reference/sig_trainmlr_tool/#arguments","text":"--ds DS : Input dataset that includes both landmarks and resulting outputs. --modeltype MODELTYPE : String. Regression model to be used: pinv_int - Linear Regression with an intercept. pinv - Linear Regression. All best fits lines pass through origin. . Default is pinv_int. Options are {pinv_int|pinv} --grp_landmark GRP_LANDMARK : GRP file containing identifiers for landmark genes. Must correspond to the row-ids in DS --dependents DEPENDENTS : GRP file containing identifiers for dependent genes to infer. Must correspond to row-ids in DS and be mutually exclusive from the landmarks. --cid CID : GRP file of column ids (samples) in DS to use for training. --xform XFORM : String. Specify any transformations to apply to data prior to training. Default is none. Options are {none|log2|abs|pow2|zscore} --precision PRECISION : Integer. Level of precision (decimal places) for output model weights.. Default is 6 --outfmt OUTFMT : Model Output format. Default is gctx. Options are {mat|gct|gctx}","title":"Arguments"},{"location":"command-reference/sig_trainmlr_tool/#description","text":"This tool takes as input a training dataset (genes x samples) containing expression values for a specified set of predictor (landmark) genes and some number of dependent genes and builds a multiple linear regression (MLR) model. The model can be applied to a new dataset using the SIG_TESTMLR_TOOL to infer the expression of the dependent genes based on landmark expression.","title":"Description"},{"location":"command-reference/sig_trainmlr_tool/#examples","text":"Create a model from training data and landmark GRP file. sig_trainmlr_tool --ds 'training_data.gctx' --grp_landmark 'landmark.grp' Create a model for a subset of dependent genes. sig_trainmlr_tool --ds 'training_data.gctx' --grp_landmark 'landmark.grp' --dependents 'subset.grp' Perform z-scoring prior to creating model. sig_trainmlr_tool --ds 'training_data.gctx' --grp_landmark 'landmark.grp' --xform 'zscore'","title":"Examples"},{"location":"command-reference/sig_tsne_tool/","text":"sig_tsne_tool \u00b6 Run T-SNE on a dataset Synopsis \u00b6 sig_tsne_tool [--ds DS] [--ds_meta DS_META] [--is_pairwise IS_PAIRWISE] [--cid CID] [--rid RID] [--row_space ROW_SPACE] [--sample_dim SAMPLE_DIM] [--out_dim OUT_DIM] [--algorithm ALGORITHM] [--initial_dim INITIAL_DIM] [--perplexity PERPLEXITY] [--theta THETA] [--missing_action MISSING_ACTION] [--missing_fill_value MISSING_FILL_VALUE] [--disable_table DISABLE_TABLE] Arguments \u00b6 --ds DS : Input dataset --ds_meta DS_META : Optional annotations as a TSV table for the input dataset for the dimension being operated on. The first column must match the corresponding id field in ds --is_pairwise IS_PAIRWISE : Handles the input dataset as a distance or similarity matrix if true. Expects the the input to be square and symmetric. Assumes the values are similarities if the main diagonal is one or distances if the main diagonal is zero. Skips the initial dimensionality reduction (via PCA) and pairwise euclidean distance computation and uses the tsne_d algorithm to perform the low-dimensional embedding. Default is 0 --cid CID : List of column ids to use specified as a GRP file or cell array. If empty all columns are used. --rid RID : List of row ids to to use specified as a GRP file or cell array. If empty all rows are used --row_space ROW_SPACE : Common row-id space definitions to use as an alternative to the rid parameter. Default is all. Options are {all|lm|bing|aig|lm_probeset|bing_probeset|full_probeset|custom} --sample_dim SAMPLE_DIM : Sample dimension of the dataset. Default is column. Options are {1|2|column|row} --out_dim OUT_DIM : Output dimensionality. Default is 2 --algorithm ALGORITHM : The t-SNE implemention to use. The standard algorithm is a native matlab implementation that is appropriate for small to moderate sized datasets and if more than 2 output dimensions are required. The Barnes Hut algorithm is a fast C++ implementation suitable for 2D tSNE representation of large datasets (>5000 samples).. Default is auto. Options are {auto|standard|barnes-hut} --initial_dim INITIAL_DIM : Initial number of PCA dimensions to use. Default is 50 --perplexity PERPLEXITY : Perplexity is a measure for information that is defined as 2 to the power of the Shannon entropy. It may be viewed as a tuning parameter that sets the number of effective nearest neighbors. It is comparable to the number of nearest neighbors k that is employed in many manifold learners. The performance of t-SNE is fairly robust under different settings of the perplexity. The most appropriate value depends on the density of the data. In general a denser dataset requires a larger perplexity. Typical values for the perplexity range between 5 and 50. Default is 30 --theta THETA : Used only in the Barnes-Hut implementation. It's a trade-off parameter to choose between speed and accuracy: theta = 0 corresponds to standard, slow t-SNE, while theta = 1 makes very crude approximations. Appropriate values for theta are between 0.1 and 0.7. Default is 0.5 --missing_action MISSING_ACTION : Action to take if data contains missing values. If 'drop' is specified the entire column (or row if sample_dim='row') is excluded prior to analysis. If 'impute' is specified the missing values are replaced by row means (or column means if sample_dim='row'). If 'fill' is specified, missing values are replaced with 'missing_fill_value'. Default is none. Options are {none|drop|impute|fill} --missing_fill_value MISSING_FILL_VALUE : Replace missing data with specified value if the 'fill' option is specified for missing_action. Default is 0 --disable_table DISABLE_TABLE : Disable generating annotated text table for first two TSNE components. The table can be generated post-hoc from the saved tsne.gctx matrix if needed.. Default is 0 Description \u00b6 Applies t-distributed stochastic neighbor embedding (t-SNE) to high dimensional datasets and returns a 2-d mapping of datapoints. t-SNE is a dimensionality reduction technique that is particularly well suited for visualization of high dimensional data in 2 or 3 dimensions. For datasets with <= 5000 samples, the standard t-SNE algorithm is used. For larger datasets the Barnes-HUT algorithm is employed. For details see http://homepage.tudelft.nl/19j49/t-SNE.html Examples \u00b6 tSNE with default parameters sig_tsne_tool --ds 'x.gctx tSNE along rows of a large dataset with >5000 rows sig_tsne_tool --ds 'large.gctx' --dim row --algorithm barnes-hut","title":"tsne"},{"location":"command-reference/sig_tsne_tool/#sig_tsne_tool","text":"Run T-SNE on a dataset","title":"sig_tsne_tool"},{"location":"command-reference/sig_tsne_tool/#synopsis","text":"sig_tsne_tool [--ds DS] [--ds_meta DS_META] [--is_pairwise IS_PAIRWISE] [--cid CID] [--rid RID] [--row_space ROW_SPACE] [--sample_dim SAMPLE_DIM] [--out_dim OUT_DIM] [--algorithm ALGORITHM] [--initial_dim INITIAL_DIM] [--perplexity PERPLEXITY] [--theta THETA] [--missing_action MISSING_ACTION] [--missing_fill_value MISSING_FILL_VALUE] [--disable_table DISABLE_TABLE]","title":"Synopsis"},{"location":"command-reference/sig_tsne_tool/#arguments","text":"--ds DS : Input dataset --ds_meta DS_META : Optional annotations as a TSV table for the input dataset for the dimension being operated on. The first column must match the corresponding id field in ds --is_pairwise IS_PAIRWISE : Handles the input dataset as a distance or similarity matrix if true. Expects the the input to be square and symmetric. Assumes the values are similarities if the main diagonal is one or distances if the main diagonal is zero. Skips the initial dimensionality reduction (via PCA) and pairwise euclidean distance computation and uses the tsne_d algorithm to perform the low-dimensional embedding. Default is 0 --cid CID : List of column ids to use specified as a GRP file or cell array. If empty all columns are used. --rid RID : List of row ids to to use specified as a GRP file or cell array. If empty all rows are used --row_space ROW_SPACE : Common row-id space definitions to use as an alternative to the rid parameter. Default is all. Options are {all|lm|bing|aig|lm_probeset|bing_probeset|full_probeset|custom} --sample_dim SAMPLE_DIM : Sample dimension of the dataset. Default is column. Options are {1|2|column|row} --out_dim OUT_DIM : Output dimensionality. Default is 2 --algorithm ALGORITHM : The t-SNE implemention to use. The standard algorithm is a native matlab implementation that is appropriate for small to moderate sized datasets and if more than 2 output dimensions are required. The Barnes Hut algorithm is a fast C++ implementation suitable for 2D tSNE representation of large datasets (>5000 samples).. Default is auto. Options are {auto|standard|barnes-hut} --initial_dim INITIAL_DIM : Initial number of PCA dimensions to use. Default is 50 --perplexity PERPLEXITY : Perplexity is a measure for information that is defined as 2 to the power of the Shannon entropy. It may be viewed as a tuning parameter that sets the number of effective nearest neighbors. It is comparable to the number of nearest neighbors k that is employed in many manifold learners. The performance of t-SNE is fairly robust under different settings of the perplexity. The most appropriate value depends on the density of the data. In general a denser dataset requires a larger perplexity. Typical values for the perplexity range between 5 and 50. Default is 30 --theta THETA : Used only in the Barnes-Hut implementation. It's a trade-off parameter to choose between speed and accuracy: theta = 0 corresponds to standard, slow t-SNE, while theta = 1 makes very crude approximations. Appropriate values for theta are between 0.1 and 0.7. Default is 0.5 --missing_action MISSING_ACTION : Action to take if data contains missing values. If 'drop' is specified the entire column (or row if sample_dim='row') is excluded prior to analysis. If 'impute' is specified the missing values are replaced by row means (or column means if sample_dim='row'). If 'fill' is specified, missing values are replaced with 'missing_fill_value'. Default is none. Options are {none|drop|impute|fill} --missing_fill_value MISSING_FILL_VALUE : Replace missing data with specified value if the 'fill' option is specified for missing_action. Default is 0 --disable_table DISABLE_TABLE : Disable generating annotated text table for first two TSNE components. The table can be generated post-hoc from the saved tsne.gctx matrix if needed.. Default is 0","title":"Arguments"},{"location":"command-reference/sig_tsne_tool/#description","text":"Applies t-distributed stochastic neighbor embedding (t-SNE) to high dimensional datasets and returns a 2-d mapping of datapoints. t-SNE is a dimensionality reduction technique that is particularly well suited for visualization of high dimensional data in 2 or 3 dimensions. For datasets with <= 5000 samples, the standard t-SNE algorithm is used. For larger datasets the Barnes-HUT algorithm is employed. For details see http://homepage.tudelft.nl/19j49/t-SNE.html","title":"Description"},{"location":"command-reference/sig_tsne_tool/#examples","text":"tSNE with default parameters sig_tsne_tool --ds 'x.gctx tSNE along rows of a large dataset with >5000 rows sig_tsne_tool --ds 'large.gctx' --dim row --algorithm barnes-hut","title":"Examples"},{"location":"command-reference/sig_zscore_tool/","text":"sig_zscore_tool \u00b6 Compute robust and conventional standardized scores Synopsis \u00b6 sig_zscore_tool [--ds DS] [--dim DIM] [--bkg_space BKG_SPACE] [--min_var MIN_VAR] [--var_adjustment VAR_ADJUSTMENT] [--estimate_prct ESTIMATE_PRCT] [--zscore_method ZSCORE_METHOD] [--use_gctx USE_GCTX] Arguments \u00b6 --ds DS : Path to input dataset --dim DIM : Choose whether to z-score along on the rows or columns. Default is row. Options are {row|column} --bkg_space BKG_SPACE : Filepath for GRP file listing a subset of column or row-ids of the input dataset that should be used to estimate the location and dispersion parameters. The ids are dependent on the '--dim' attribute, if dim is row, column-ids should be specified, otherwise row-ids should be used. Default is to use the all values in the row (or column) as the background. --min_var MIN_VAR : Minimum variation as estimated by MAD or standard deviation for robust and standard zscore methods respectively. Default is 0.1 --var_adjustment VAR_ADJUSTMENT : Adjustment for low variance. Valid options are: 'estimate' - estimate the minimum variance from the data using: EST_VAR = PRCTILE(SIGMA, est_prct), where SIGMA is a vector of variation (e.g. MAD, stdev) computed along dim for the entire dataset. The minimum variance is set to MAX(EST_VAR, MIN_VAR); 'fixed' - Default value. Use 'min_var' for minimum variance. 'none' - Assume min_var is zero . Default is fixed. Options are {estimate|fixed|none} --estimate_prct ESTIMATE_PRCT : Percentile to consider when estimating the minimum variance from the data. Only used when '--var_adjustment' is set set to 'estimate'. Default is 1 --zscore_method ZSCORE_METHOD : Determines the type of z-scoring is performed. 'robust' - uses median and median absolute deviation (MAD) to calculate z-scores as (X-NANMEDIAN(X))./(MAD(X)*1.4826) 'standard' - uses the mean and std to calculate z-scores as (X-NANMEAN(X))./ NANSTD(X). Default is robust. Options are {robust|standard} --use_gctx USE_GCTX : Save file in binary GCTX format if 1 else save as a text GCT. Default is 1. Options are {1,0} Description \u00b6 The sig_zscore_tool returns a z-scored version on the input matrix X, the same dimensions as X. Two methods for computing Z-scores are currently supported. The default is to compute a robust z-score as: (X-NANMEDIAN(X))./(MAD(X)*1.4826). Alternatively a standard zscore can be specified and is computed as: (X-NANMEAN(X))./ NANSTD(X). In addition adjustments are made to account for low variance based on heuristics controlled via the var_adjustment parameter. Examples \u00b6 Run row-wise z-scoring using the entire dataset sig_zscore_tool --ds 'raw_data.gctx' Run column-wise z-scoring using a custom background space sig_zscore_tool --ds 'raw_data.gctx' --dim 'column' --bkg_space 'rowids.grp'","title":"zscore"},{"location":"command-reference/sig_zscore_tool/#sig_zscore_tool","text":"Compute robust and conventional standardized scores","title":"sig_zscore_tool"},{"location":"command-reference/sig_zscore_tool/#synopsis","text":"sig_zscore_tool [--ds DS] [--dim DIM] [--bkg_space BKG_SPACE] [--min_var MIN_VAR] [--var_adjustment VAR_ADJUSTMENT] [--estimate_prct ESTIMATE_PRCT] [--zscore_method ZSCORE_METHOD] [--use_gctx USE_GCTX]","title":"Synopsis"},{"location":"command-reference/sig_zscore_tool/#arguments","text":"--ds DS : Path to input dataset --dim DIM : Choose whether to z-score along on the rows or columns. Default is row. Options are {row|column} --bkg_space BKG_SPACE : Filepath for GRP file listing a subset of column or row-ids of the input dataset that should be used to estimate the location and dispersion parameters. The ids are dependent on the '--dim' attribute, if dim is row, column-ids should be specified, otherwise row-ids should be used. Default is to use the all values in the row (or column) as the background. --min_var MIN_VAR : Minimum variation as estimated by MAD or standard deviation for robust and standard zscore methods respectively. Default is 0.1 --var_adjustment VAR_ADJUSTMENT : Adjustment for low variance. Valid options are: 'estimate' - estimate the minimum variance from the data using: EST_VAR = PRCTILE(SIGMA, est_prct), where SIGMA is a vector of variation (e.g. MAD, stdev) computed along dim for the entire dataset. The minimum variance is set to MAX(EST_VAR, MIN_VAR); 'fixed' - Default value. Use 'min_var' for minimum variance. 'none' - Assume min_var is zero . Default is fixed. Options are {estimate|fixed|none} --estimate_prct ESTIMATE_PRCT : Percentile to consider when estimating the minimum variance from the data. Only used when '--var_adjustment' is set set to 'estimate'. Default is 1 --zscore_method ZSCORE_METHOD : Determines the type of z-scoring is performed. 'robust' - uses median and median absolute deviation (MAD) to calculate z-scores as (X-NANMEDIAN(X))./(MAD(X)*1.4826) 'standard' - uses the mean and std to calculate z-scores as (X-NANMEAN(X))./ NANSTD(X). Default is robust. Options are {robust|standard} --use_gctx USE_GCTX : Save file in binary GCTX format if 1 else save as a text GCT. Default is 1. Options are {1,0}","title":"Arguments"},{"location":"command-reference/sig_zscore_tool/#description","text":"The sig_zscore_tool returns a z-scored version on the input matrix X, the same dimensions as X. Two methods for computing Z-scores are currently supported. The default is to compute a robust z-score as: (X-NANMEDIAN(X))./(MAD(X)*1.4826). Alternatively a standard zscore can be specified and is computed as: (X-NANMEAN(X))./ NANSTD(X). In addition adjustments are made to account for low variance based on heuristics controlled via the var_adjustment parameter.","title":"Description"},{"location":"command-reference/sig_zscore_tool/#examples","text":"Run row-wise z-scoring using the entire dataset sig_zscore_tool --ds 'raw_data.gctx' Run column-wise z-scoring using a custom background space sig_zscore_tool --ds 'raw_data.gctx' --dim 'column' --bkg_space 'rowids.grp'","title":"Examples"}]}